[{"id":0,"href":"/www6vMLSys/docs/LLMOps/GPU/GPUk8s/","title":"(实战)K8s部署GPU","section":"GPU","content":"\nK8s部署GPU # (实战)K8s部署GPU\n"},{"id":1,"href":"/www6vMLSys/docs/LLMOps/AI%E5%B9%B3%E5%8F%B0/AI%E7%BD%91%E5%85%B3/LiteLLM/","title":"(实现)LiteLLM","section":"AI网关","content":" Arch[1] # Routing and Load Balancing[2] # 参考 # 快速上手 LiteLLM：打造高效、稳定、面向生产的 LLM 应用程序\ncode # https://deepwiki.com/BerriAI/litellm/3-litellm-proxy https://deepwiki.com/BerriAI/litellm/3.4-routing-and-load-balancing https://github.com/BerriAI/litellm\nhttps://github.com/BerriAI/litellm/blob/c946ae85/litellm/router.py\nhttps://github.com/BerriAI/litellm/blob/c946ae85/litellm/types/router.py\nhttps://github.com/BerriAI/litellm/blob/c946ae85/litellm/proxy/proxy_server.py\ndoc # https://docs.litellm.ai/docs/\nhttps://docs.litellm.ai/docs/routing\n"},{"id":2,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Practice/gptQuantizationPractice/","title":"(实战)量化 +","section":"Practice","content":"\n量化实战 # (实战)量化-推理\n"},{"id":3,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainMegatron/","title":"(原理)Megatron +","section":"混合并行","content":"\nMegatron # (原理)Megatron\n"},{"id":4,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantizationSurvey/","title":"(Survey)Quantization +","section":"Overview","content":"\nQuantization # A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms\n"},{"id":5,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationGPTQ/","title":"(原理|实战|实现)GPTQ +","section":"Weight Only","content":"\nGPTQ # (原理|实战|实现)GPTQ\n"},{"id":6,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationInt8/gptQuantizationInt8/","title":"(原理|实战)LLM.int8() +","section":"Weight\u0026Activation","content":"\nLLM.int8() # (原理|实战)LLM.int8()\n"},{"id":7,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/","title":"Speculative Decoding +","section":"SpeculativeDecoding","content":"\nSpeculative Decoding # Speculative Decoding\n"},{"id":8,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/","title":"DistServe +","section":"PD 分离","content":"\nDistServe # DistServe\n"},{"id":9,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/","title":"Continuous Batching +","section":"Batch","content":"\nContinuous Batching # Continuous Batching\n"},{"id":10,"href":"/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/LowPrecision/gptLowPrecision/","title":"低精度训练 +","section":"低精度","content":"\n低精度训练 # 低精度训练\n"},{"id":11,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptFlashAttention/","title":"(原理)Flash Attention +","section":"FlashAttention","content":"\nFlash Attention # (原理)Flash Attention\n"},{"id":12,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/","title":"(原理|实现) KV Cache +","section":"KVCache","content":"\nKV Cache # (原理|实现) KV Cache\n"},{"id":13,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/","title":"(原理) vLLM  +","section":"框架vLLM","content":"\nvLLM # (原理) vLLM\n"},{"id":14,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/gptQuantizationWeight/","title":"(原理)PTQ-Weight Only +","section":"PTQ","content":"\nWeight Only # Weight Only\n"},{"id":15,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainZeroDeepspeed/","title":"(原理) Deepspeed Zero +","section":"DP","content":"\nDeepspeed Zero # (原理) Deepspeed Zero\n"},{"id":16,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/","title":"(原理)推理-框架","section":"框架","content":"\n推理 框架[1] # inference execute engine(server)\nvLLM，TensorRT， deepspeed\ninference execute engine(pc/edge 移动端)\nllama.cpp\nmlc-llm\nollama\ninference Server\nTriton Server, Ray\nChat Server [2]\nFastChat, XInference, modelscope SWIFT\n参考 # 探秘LLM应用开发 8-19\nLLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference/FastChat等框架]\n1xx. 一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)\n1xx. 一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) 1xx. 大模型推理框架概述\n"},{"id":17,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInference/","title":"(总结)推理优化","section":"Overview","content":"\n推理 优化 # overview[2] # 有几种方法可以在内存中降低推理成本或/和加快推理速度。\n应用各种并行处理方式，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。 内存卸载，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。 智能批处理策略；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。 网络压缩技术，如修剪、量化、蒸馏。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。 针对目标模型架构的特定改进。许多架构变化，特别是针对注意力层的变化，有助于提高Transformer解码速度。 模型压缩 [1] # 剪枝（Pruning） 知识蒸馏（Knowledge Distillation，KD） 量化（Quantization） 低秩分解（Low-Rank Factorization） KV Cache # 参考 # 综述 # 一文探秘LLM应用开发(13)-模型部署与推理(优化理论) Large Transformer Model Inference Optimization lilianweng 1xx. NLP（十八）：LLM 的推理优化技术纵览 ***\n1xx. 大语言模型推理性能优化综述\n"},{"id":18,"href":"/www6vMLSys/docs/LLMOps/AI%E5%B9%B3%E5%8F%B0/aiPlatform/","title":"(原理)AI平台","section":"AI平台","content":"\nStep 2. Put in Guardrails # Input guardrails\nOutput guardrails\nGuardrail tradeoffs\nStep 3. Add Model Router and Gateway # Router\nGateway\nStep 4. Reduce Latency with Cache # Prompt cache\nExact cache\nSemantic cache\nObservability # Metrics # Time to First Token (TTFT): The time it takes for the first token to be generated. Time Between Tokens (TBT): The interval between each token generation. Tokens Per Second (TPS): The rate at which tokens are generated. Time Per Output Token (TPOT): The time it takes to generate each output token. Total Latency: The total time required to complete a response. 参考 # Building A Generative AI Platform\n【译】构建生成式 AI 平台概述\n聊聊AI应用架构演进\n"},{"id":19,"href":"/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/Precision/gptPrecision/","title":"(原理|实战)混合精度 +","section":"低精度","content":"\n混合精度 # (原理|实战)混合精度\n"},{"id":20,"href":"/www6vMLSys/docs/LLMOps/GPU/GPUMetrics/","title":"GPU 指标\u0026监控","section":"GPU","content":"\nGPU 指标\u0026amp;监控 # GPU 指标\u0026amp;监控\n"},{"id":21,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainHybridParallel/","title":"(原理)混合并行 +","section":"混合并行","content":"\n混合并行 # (原理)混合并行\n"},{"id":22,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/","title":"(原理)FlashAttention2 +","section":"FlashAttention","content":"\nFlash Attention2 # (原理)Flash Attention2\n"},{"id":23,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/","title":"(Survey)Speculative Decoding +","section":"SpeculativeDecoding","content":"\nSpeculative Decoding # (Survey)Speculative Decoding "},{"id":24,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/","title":"(原理|实现)Mooncake +","section":"PD 分离","content":"\nMooncake # Mooncake\n"},{"id":25,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationSmoothQuant/gptQuantizationSmoothQuant/","title":"(原理)SmoothQuant +","section":"Weight\u0026Activation","content":"\nSmoothQuant # (原理)SmoothQuant\n"},{"id":26,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationAWQ/","title":"(原理|实战)AWQ +","section":"Weight Only","content":"\nAWQ # (原理|实战)AWQ\n"},{"id":27,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/","title":"Chunked Prefill +","section":"Batch","content":"\nChunked Prefill # Chunked Prefill\n"},{"id":28,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/","title":"(原理)KV Cache 优化 +","section":"KVCache","content":"\nKV Cache 优化 # (原理)KV cache优化\n"},{"id":29,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/","title":"(综述)推理优化 +","section":"Overview","content":"\n论文 # A Survey on Efficient Inference for Large Language Models 翻译\nA Survey on Efficient Inference for Large Language Models 总结\nInference Papers # Inference Papers\n"},{"id":30,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/","title":"(实战) vLLM +","section":"框架vLLM","content":"\nvLLM 实战 # (实战) vLLM\n"},{"id":31,"href":"/www6vMLSys/docs/LLMOps/LLamaFactory/","title":"LLama-Factory","section":"LLMOps","content":"\nLLama-Factory # LLama-Factory\n"},{"id":32,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDistributedPractice/","title":"(实战)DeepSpeed Training +","section":"DP","content":"\nDeepSpeed Training # DeepSpeed Training\n"},{"id":33,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantization/","title":"(原理)量化 +","section":"Overview","content":"\n量化 # 量化\n"},{"id":34,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/","title":"(实战)推理-lmdeploy","section":"框架","content":"\nlmdeploy-推理部署 [10] # 模型转换 # TurboMind 推理+命令行本地对话 # TurboMind推理+API服务 # 启动服务 Client访问服务 参考 # lmdeploy 量化部署\n(5)LMDeploy 大模型量化部署实践 V 1xx. llm-action inference git\n"},{"id":35,"href":"/www6vMLSys/docs/LLMOps/AI%E5%B9%B3%E5%8F%B0/AI%E7%BD%91%E5%85%B3/aiGateway/","title":"(阿里) AI 网关","section":"AI网关","content":" AI 网关简述 # 阿里云AI网关和阿里云云原生API网关同属一个内核。 AI 网关的能力主要包括六部分：\n模型服务管理：可以代理市面上所有主流的模型托管服务，以及兼容 OpenAI 协议的 LLM 服务和多模态 LLM 服务。在这个模块中包括协议转换、多 API Key 管理、Fallback、多模型切换等多个核心功能。\nMCP 管理：负责 MCP 服务的代理以及 MCP 服务的策略管理。包括代理原生 MCP 服务，HTTP 服务转 MCP 服务，MCP 服务鉴权认证，和 MSE Nacos 集成实现从 MCP Registry 自动发现 MCP 服务。\nAgent 管理：负责 Agent 的代理以及 Agent 的策略管理。目前支持代理百炼 Agent，Dify 构建的 Agent（流程），AIStudio 构建的 Agent（流程），自定义 Agent。\nAI 安全防护：安全防护分为三个层面，一个是输入输出的内容安全防护，另一个是保护下游 LLM 服务的稳定，以及管控 AI 接口消费者。在这个模块中包括内容审核、基于 Token 的限流降级、消费者认证等多个核心功能。\nAI 插件：AI 网关的灵活扩展机制我们使用插件的形式来实现，目前有很多预置的插件，用户也可以开发自定义插件来丰富 AI 场景流量的管控。比如基于 AI 插件机制我们实现了结果缓存、提示词装饰器、向量检索等能力。\nAI 可观测：AI 场景的可观测和传统场景的可观测是有很大区别的，监控和关注的指标都是不同的，云原生 AI 网关结合阿里云日志服务和可观测产品实现了贴合 AI 应用业务语义的可观测模块和 AI 观测大盘，支持比如 Tokens 消费观测，流式/非流式的 RT，首包 RT，缓存命中等可观指标。同时所有的输入输出 Tokens 也都记录在日志服务 SLS 中，可供用户做更详细的分析。\n参考 # 企业AI落地实践（三）：使用 AI 网关解决 AI Agent 与 LLM 的交互挑战\n"},{"id":36,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/","title":"(综述)推理优化 +","section":"Overview","content":"\n论文 # Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems\n"},{"id":37,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/","title":"(原理)Llumnix +","section":"PD 分离","content":"\nLlumnix # (原理)Llumnix\n"},{"id":38,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/","title":"(实战)[vLLM]投机解码 +","section":"框架vLLM","content":"\nSpeculating with a draft model[1] # from vllm import LLM, SamplingParams prompts = [ \u0026#34;The future of AI is\u0026#34;, ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) llm = LLM( model=\u0026#34;facebook/opt-6.7b\u0026#34;, # verify 小模型 tensor_parallel_size=1, speculative_model=\u0026#34;facebook/opt-125m\u0026#34;, # draft 小模型 num_speculative_tokens=5, # 一次生成5个token ) outputs = llm.generate(prompts, sampling_params) for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\u0026#34;Prompt: {prompt!r}, Generated text: {generated_text!r}\u0026#34;) python -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8000 \\ --model facebook/opt-6.7b \\ --seed 42 -tp 1 \\ --speculative_model facebook/opt-125m \\ --use-v2-block-manager \\ --num_speculative_tokens 5 \\ --gpu_memory_utilization 0.8 \\ Small LM\nDistillSpec\nSpeculating by matching n-grams in the prompt[1,2] # from vllm import LLM, SamplingParams prompts = [ \u0026#34;The future of AI is\u0026#34;, ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) llm = LLM( model=\u0026#34;facebook/opt-6.7b\u0026#34;, tensor_parallel_size=1, speculative_model=\u0026#34;[ngram]\u0026#34;, # ngram num_speculative_tokens=5, # 一次生成5个token ngram_prompt_lookup_max=4, ) outputs = llm.generate(prompts, sampling_params) for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\u0026#34;Prompt: {prompt!r}, Generated text: {generated_text!r}\u0026#34;) lookahead Speculating using MLP speculators[1] # from vllm import LLM, SamplingParams prompts = [ \u0026#34;The future of AI is\u0026#34;, ] sampling_params = SamplingParams(temperature=0.8, top_p=0.95) llm = LLM( model=\u0026#34;meta-llama/Meta-Llama-3.1-70B-Instruct\u0026#34;, tensor_parallel_size=4, speculative_model=\u0026#34;ibm-fms/llama3-70b-accelerator\u0026#34;, speculative_draft_tensor_parallel_size=1, ) outputs = llm.generate(prompts, sampling_params) for output in outputs: prompt = output.prompt generated_text = output.outputs[0].text print(f\u0026#34;Prompt: {prompt!r}, Generated text: {generated_text!r}\u0026#34;) Medusa 参考 # Speculative decoding in vLLM 3种类型\nWhat is Lookahead Scheduling in vLLM?\n1xx. A Hacker’s Guide to Speculative Decoding in vLLM v ***\nA Hacker’s Guide to Speculative Decoding in vLLM pdf\n1xx. Optimizing attention for spec decode can reduce latency / increase throughput\n"},{"id":39,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/","title":"(原理|实现)Medusa +","section":"SpeculativeDecoding","content":"\nMedusa # (原理|实现)Medusa\n"},{"id":40,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationFP8/gptQuantizationFP8/","title":"(原理)FP8 +","section":"Weight\u0026Activation","content":"\nFP8 # (原理)FP8\n"},{"id":41,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/","title":"(原理)Flash Decoding +","section":"FlashAttention","content":"\nFlash Decoding # (原理)Flash Decoding\n"},{"id":42,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDDP/","title":"(原理|实战)DDP +","section":"DP","content":"\nDDP # DDP\n"},{"id":43,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/","title":"(原理)KV Cache 量化 +","section":"Compress","content":"\nKV Cache 量化 # KV Cache 量化\n"},{"id":44,"href":"/www6vMLSys/docs/LLMOps/GPUComputing/","title":"显存估算","section":"LLMOps","content":"\n显存估算 # 显存估算\n"},{"id":45,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/","title":"(原理|实战) TensorRT-LLM +","section":"框架","content":"\nTensorRT-LLM # (原理|实战) TensorRT-LLM\n"},{"id":46,"href":"/www6vMLSys/docs/LLMOps/GPU/GPU/","title":"GPU 算力平台","section":"GPU","content":"\nGPU算力 # 免费[1] # modelscope 100小时 GPU 专业收费[2] # 显卡 # 显卡天梯榜 显卡天梯榜\n显卡 显卡 = GPU + 显存\n参考 # 5种在线GPU算力资源白嫖指南 V 5种专业在线GPU算力资源白嫖指南 V.\n1xx. 【PyTorch深度学习】01 GPU购买与白嫖指南 "},{"id":47,"href":"/www6vMLSys/docs/LLMOps/AI%E5%B9%B3%E5%8F%B0/aiObserve/","title":"(阿里)AI 应用观测","section":"AI平台","content":" AI 应用典型框架 # AI 全栈统一观测 # 最佳实践 # Trace 全诊断能力 # Dify 生产级部署应用 # todo\n基于大模型生成结果评估 # todo\n参考 # 【深度】企业 AI 落地实践（四）：如何构建端到端的 AI 应用观测体系\n"},{"id":48,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/","title":"(原理|实战) [vLLM]Prefix Cache +","section":"框架vLLM","content":"\n(原理|实战) [vLLM]Prefix Cache # (原理|实战) [vLLM]Prefix Cache\n"},{"id":49,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/","title":"EAGLE +","section":"SpeculativeDecoding","content":"\nEAGLE # EAGLE\n"},{"id":50,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainFSDP/","title":"(原理|实战)FSDP +","section":"DP","content":"\nFSDP # FSDP\n"},{"id":51,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/","title":"(原理)推理 Ray","section":"框架","content":"\nArchitecture Overview # Application concepts [1] # Task - A remote function invocation. Object - An application value. Actor - a stateful worker process (an instance of a @ray.remote class). Driver - The program root, or the “main” program. Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment. Design [1] # Components One or more worker processes A raylet. scheduler object store head node Global Control Service (GCS) driver process(es) cluster-level services Spark vs. Ray[10] # 总的来说，Ray和Spark的主要差别在于他们的抽象层次。Spark对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。Ray的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，Ray揭示和暴露了并行，而Spark抽象和隐藏了并行。\n就架构而言，Spark采用BSP模型，是无副作用的，而Ray本质上是一个RPC 框架+Actor框架+对象存储。\n参考 # 1xx. 基于 Ray 的大规模离线推理 字节\n字节跳动基于 Ray 的大规模离线推理\n1xx. Ray Design Patterns 查看-\u0026gt;模式\n1xx. 大模型训练部署利器\u0026ndash;开源分布式计算框架Ray原理介绍\nSpark vs. Ray # 加州大学伯克利分校为何能连续孵化出 Mesos,Spark,Alluxio,Ray 等重量级开源项目? 孙挺Sunt 1xx. 分布式领域计算模型及Spark\u0026amp;Ray实现对比\nInternal # Ray v2 Architecture 1xx. Ray 分布式计算框架介绍\n1xx. Ray 1.0 架构解读\n"},{"id":52,"href":"/www6vMLSys/docs/LLMOps/LLMOps/","title":"LLMOps","section":"LLMOps","content":"\nLLMOps: Deployment and Learning in Production\nLLMOps: Deployment and Learning in Production\n[必读] LLM 应用开发全栈指南 LLMOps 了解一下新领域 LLMOps: 大模型运维\nUnderstanding LLMOps: Large Language Model Operations "},{"id":53,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/","title":"SpecInfer +","section":"SpeculativeDecoding","content":"\nSpecInfer # SpecInfer\n"},{"id":54,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/","title":"(实战)推理 Ray","section":"框架","content":"\n实战 # 环境 # modelscope GPU\n实战1 # 脚本[1]\n遇到的异常[2]\n实战2 # 脚本 ### 变更模型名字 ### import \u0026#39;modelscope\u0026#39; package 异常[11] 实战3[20] # 脚本\nvllm 0.2.3 -\u0026gt; 报异常\nvllm 0.3.3 -\u0026gt; 报另一个异常 实战4 # 脚本 [30]\n异常 [31]\n# 运行这个命令报异常 python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000 monitor[40] # Ray Dashboard[41] # Ray logging # Loki grafana\nBuilt-in Ray Serve metrics # Prometheus\n参考 # 实战1 # Serve a Large Language Model with vLLM\nInvalid device id when using pytorch dataparallel！ 运行时碰到的异常\n实战2 # examples/offline_inference_distributed.py\n报错:RuntimeError: CUDA error: no kernel image is available for execution on the device\n实战3 # Ray vLLM Interence 1xx. GitHub - ray-project/langchain-ray: Examples on how to use LangChain and Ray git\n实战4 # 在甲骨文云上用 Ray +Vllm 部署 Mixtral 8*7B 模型_mixtral 8x7b 部署-CSDN博客\n报错:RuntimeError: CUDA error: no kernel image is available for execution on the device-CSDN博客\nmonitor # Monitor Your Application\nRay Dashboard "},{"id":55,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/","title":"(实现)[vLLM]整体架构 +","section":"框架vLLM","content":"\nvLLM # (实现)[vLLM]整体架构\n"},{"id":56,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/","title":"(实现)[vLLM]分布式 *","section":"框架vLLM","content":"\nFeature # Distributed Inference\nWhy distributed inference?\nInfra-side\nCommunication device:\nNVLink: direct communication between GPUs Infinity Band: High-speed connection between nodes RDMA: Remote direct memory access RDMA NIC Software solution Key advantage: bypass operating system / zero copy Communication library:\ndlms/distributed/device_communicators PyDCL: communication for NVIDIA shared memory : OS custom allreduce - A kernel jsut for all reduce operation Before: 0 machine: [0] 1 machine: [1] 2 machine: [2] 3 machine: [3] After: 0 machine: [0,1,2,3] 1 machine: [0,1,2,3] 2 machine: [0,1,2,3] 3 machine: [0,1,2,3] torch.distributed : provide wide support to a list of communication library GroupCoordinator\nAlgorithm-side\n[TP] vlms/model_executor/models/llama.py Pipeline parallel\nMuch less requirement to device\u0026ndash;device connection hardware Cost: not improve latency Tensor parallel: directly improve latency Algorithm-side: Worker in charge of a subset of layers ~~vlms/model_executor/models/llama.py~~ vlms/model_executor/models/llama.py self.start_layer \u0026ndash;\u0026gt; self.end_layer between workers: communicate IntermediateTensor get_pp_group() vlms/worker/model_runner.py: search get_pp_group() Expert parallel \u0026amp; data parallel (advanced)\nWhy expert parallel: Mistral / Mixtral / Deepseek model: Mixture of Experts (MoE) Only for linear layers Normal MoE: all weights participant in computation MoE: expert as granularity, only a small subset of experts participate the computation, this subset of experts may be different between request Place different experts onto different GPUs \u0026ndash;\u0026gt; expert parallel Algorithm: Expert parallel: Shuffle (deepsp communication kernel) Forward Shuffle back TP is for attention, EP is for linear layers. Shared expert will have high load \u0026ndash;\u0026gt; duplicate shared expert. DP (data parallel)\nmax tp \u0026laquo; ep needed tp \u0026lt; # attention head basic linear layer \u0026ldquo;degree of parallism\u0026rdquo; \u0026raquo; basic attention layer tp \u0026ldquo;degree of parallism\u0026rdquo;, parallel request to raise attention \u0026ldquo;degree of parallism\u0026rdquo; Difficult to implement in practice: request padding to avoid deadlock. Types of distributed inference: TP / PP / EP / DP\nPD Disaggregation\n代码 # TP # https://github.com/vllm-project/vllm/blob/main/vllm/distributed/parallel_state.py\n_TP: Optional[GroupCoordinator] = None ### TP def get_tp_group() -\u0026gt; GroupCoordinator: assert _TP is not None, (\u0026#34;tensor model parallel group is not initialized\u0026#34;) return _TP class GroupCoordinator: \u0026#34;\u0026#34;\u0026#34; PyTorch ProcessGroup wrapper for a group of processes. PyTorch ProcessGroup is bound to one specific communication backend, e.g. NCCL, Gloo, MPI, etc. GroupCoordinator takes charge of all the communication operations among the processes in the group. It manages both CPU and device communication. \u0026#34;\u0026#34;\u0026#34; # available attributes: rank: int # global rank ranks: list[int] # global ranks in the group world_size: int # size of the group # difference between `local_rank` and `rank_in_group`: # if we have a group of size 4 across two nodes: # Process | Node | Rank | Local Rank | Rank in Group # 0 | 0 | 0 | 0 | 0 # 1 | 0 | 1 | 1 | 1 # 2 | 1 | 2 | 0 | 2 # 3 | 1 | 3 | 1 | 3 local_rank: int # local rank used to assign devices rank_in_group: int # rank inside the group cpu_group: ProcessGroup # group for CPU communication device_group: ProcessGroup # group for device communication use_device_communicator: bool # whether to use device communicator device_communicator: DeviceCommunicatorBase # device communicator mq_broadcaster: Optional[Any] # shared memory broadcaster https://github.com/vllm-project/vllm/blob/main/vllm/distributed/device_communicators/pynccl.py\ndef all_reduce(self, in_tensor: torch.Tensor, op: ReduceOp = ReduceOp.SUM, stream=None) -\u0026gt; torch.Tensor: if self.disabled: return None # nccl communicator created on a specific device # will only work on tensors on the same device # otherwise it will cause \u0026#34;illegal memory access\u0026#34; assert in_tensor.device == self.device, ( f\u0026#34;this nccl communicator is created to work on {self.device}, \u0026#34; f\u0026#34;but the input tensor is on {in_tensor.device}\u0026#34;) out_tensor = torch.empty_like(in_tensor) if stream is None: stream = current_stream() self.nccl.ncclAllReduce(buffer_type(in_tensor.data_ptr()), buffer_type(out_tensor.data_ptr()), in_tensor.numel(), ncclDataTypeEnum.from_torch(in_tensor.dtype), ncclRedOpTypeEnum.from_torch(op), self.comm, cudaStream_t(stream.cuda_stream)) return out_tensor def all_gather(self, output_tensor: torch.Tensor, input_tensor: torch.Tensor, stream=None): if self.disabled: return # nccl communicator created on a specific device # will only work on tensors on the same device # otherwise it will cause \u0026#34;illegal memory access\u0026#34; assert input_tensor.device == self.device, ( f\u0026#34;this nccl communicator is created to work on {self.device}, \u0026#34; f\u0026#34;but the input tensor is on {input_tensor.device}\u0026#34;) if stream is None: stream = current_stream() self.nccl.ncclAllGather( buffer_type(input_tensor.data_ptr()), buffer_type(output_tensor.data_ptr()), input_tensor.numel(), ncclDataTypeEnum.from_torch(input_tensor.dtype), self.comm, cudaStream_t(stream.cuda_stream)) TP in llama # https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llama.py\nclass LlamaAttention(nn.Module): def __init__( self, config: LlamaConfig, hidden_size: int, num_heads: int, num_kv_heads: int, rope_theta: float = 10000, rope_scaling: Optional[dict[str, Any]] = None, max_position_embeddings: int = 8192, quant_config: Optional[QuantizationConfig] = None, bias: bool = False, bias_o_proj: bool = False, cache_config: Optional[CacheConfig] = None, prefix: str = \u0026#34;\u0026#34;, attn_type: str = AttentionType.DECODER, ) -\u0026gt; None: super().__init__() layer_idx = extract_layer_index(prefix) self.hidden_size = hidden_size tp_size = get_tensor_model_parallel_world_size() ### self.total_num_heads = num_heads assert self.total_num_heads % tp_size == 0 self.num_heads = self.total_num_heads // tp_size ### self.total_num_kv_heads = num_kv_heads if self.total_num_kv_heads \u0026gt;= tp_size: # Number of KV heads is greater than TP size, so we partition # the KV heads across multiple tensor parallel GPUs. assert self.total_num_kv_heads % tp_size == 0 else: # Number of KV heads is less than TP size, so we replicate # the KV heads across multiple tensor parallel GPUs. assert tp_size % self.total_num_kv_heads == 0 self.num_kv_heads = max(1, self.total_num_kv_heads // tp_size) ### # MistralConfig has an optional head_dim introduced by Mistral-Nemo head_dim = getattr(config, \u0026#34;head_dim\u0026#34;, None) if head_dim is None: head_dim = self.hidden_size // self.total_num_heads self.head_dim = head_dim # Phi models introduced a partial_rotary_factor parameter in the config self.partial_rotary_factor = getattr(config, \u0026#34;partial_rotary_factor\u0026#34;, 1) self.q_size = self.num_heads * self.head_dim self.kv_size = self.num_kv_heads * self.head_dim self.scaling = self.head_dim**-0.5 self.rope_theta = rope_theta self.max_position_embeddings = max_position_embeddings PP # @support_torch_compile class LlamaModel(nn.Module): def __init__(self, *, vllm_config: VllmConfig, prefix: str = \u0026#34;\u0026#34;, layer_type: type[nn.Module] = LlamaDecoderLayer): super().__init__() config = vllm_config.model_config.hf_config cache_config = vllm_config.cache_config quant_config = vllm_config.quant_config lora_config = vllm_config.lora_config self.config = config self.quant_config = quant_config lora_vocab = (lora_config.lora_extra_vocab_size * (lora_config.max_loras or 1)) if lora_config else 0 self.vocab_size = config.vocab_size + lora_vocab self.org_vocab_size = config.vocab_size if get_pp_group().is_first_rank or (config.tie_word_embeddings and get_pp_group().is_last_rank): self.embed_tokens = VocabParallelEmbedding( self.vocab_size, config.hidden_size, org_num_embeddings=config.vocab_size, quant_config=quant_config, ) else: self.embed_tokens = PPMissingLayer() self.start_layer, self.end_layer, self.layers = make_layers( ## start_layer end_layer config.num_hidden_layers, lambda prefix: layer_type(config=config, cache_config=cache_config, quant_config=quant_config, prefix=prefix), prefix=f\u0026#34;{prefix}.layers\u0026#34;, ) if get_pp_group().is_last_rank: self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps) else: self.norm = PPMissingLayer() self.aux_hidden_state_layers: tuple[int] = tuple() self.make_empty_intermediate_tensors = ( make_empty_intermediate_tensors_factory( [\u0026#34;hidden_states\u0026#34;, \u0026#34;residual\u0026#34;], config.hidden_size)) 参考 # [EP02]分布式推理优化，vllm源码解读\n"},{"id":57,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/","title":"(实现)[vLLM]PD分离 *","section":"框架vLLM","content":"\nPD disaggregation # What\u0026rsquo;s Prefill and Decode # prefill: process input prompt, generate KV cache decode: generate tokens based on the KV cache Why PD disaggregation # Prefill:\nattention — N tokens QKV — generate KV cache takes a long time Decode:\nattention N KV, 1 Q — generate a new token very fast initial logic\nprioritize prefill\nproblem\nprefill will stop other request\u0026rsquo;s decode\nsolution\nPD disaggregation, chunked prefill\nHow to extract (and inject) KV cache from (to) vLLM * # connector API\ncalled in model_runner\nbefore model forward\ntry receive KV cache (inject KV cache into vLLM\u0026rsquo;s paged memory)\nmodel forward\nafter model forward\nextract KV cache from vLLM\u0026rsquo;s paged memory and send it to outside\nWhen to send the request to P and D node # first P then D first D then P 代码 # https://github.com/vllm-project/vllm/blob/main/vllm/worker/model_runner.py\n【inject KV cache into vLLM\u0026#39;s paged memory】 # Receive KV cache in distributed KV cache transfer setting # In disagg prefill setting, it will also recv hidden states and bypass # model forwarding # In KV cache database setting, it will change the model input so that # we can skip prefilling on tokens that successfully received KV caches # NOTE: The receive operation is blocking bypass_model_exec = False if self.need_recv_kv(model_input, kv_caches): hidden_or_intermediate_states, bypass_model_exec, model_input = \\\\ get_kv_transfer_group().recv_kv_caches_and_hidden_states( # model is used to know which layer the current worker # is working on, so that we can receive KV for only those # layers. model_executable, model_input, kv_caches=kv_caches ) multi_modal_kwargs = model_input.multi_modal_kwargs or {} seqlen_agnostic_kwargs = { \u0026#34;finished_requests_ids\u0026#34;: model_input.finished_requests_ids, \u0026#34;request_ids_to_seq_ids\u0026#34;: model_input.request_ids_to_seq_ids, } if self.has_inner_state else {} model_kwargs = {} if previous_hidden_states is not None: model_kwargs[\u0026#34;previous_hidden_states\u0026#34;] = previous_hidden_states if (self.observability_config is not None and self.observability_config.collect_model_forward_time): model_forward_start = torch.cuda.Event(enable_timing=True) model_forward_end = torch.cuda.Event(enable_timing=True) model_forward_start.record() if not bypass_model_exec: with set_forward_context(model_input.attn_metadata, self.vllm_config, virtual_engine): hidden_or_intermediate_states = model_executable( input_ids=model_input.input_tokens, inputs_embeds=model_input.inputs_embeds, positions=model_input.input_positions, intermediate_tensors=intermediate_tensors, **MultiModalKwargs.as_kwargs(multi_modal_kwargs, device=self.device), **seqlen_agnostic_kwargs, **model_kwargs, ) if (self.observability_config is not None and self.observability_config.collect_model_forward_time): model_forward_end.record() 【extract KV cache from vLLM paged memory and send it to outside】 # Sending KV cache in distributed KV cache transfer setting # NOTE: the send operation is non-blocking if self.need_send_kv(model_input, kv_caches): get_kv_transfer_group().send_kv_caches_and_hidden_states( # model_executable is used to know which layer the current # worker is working on, so that we can send KV for only those # layers. model_executable, model_input, kv_caches, hidden_or_intermediate_states, ) https://github.com/vllm-project/vllm/blob/main/vllm/distributed/kv_transfer/kv_connector/simple_connector.py\ndef send_kv_caches_and_hidden_states( self, model_executable: torch.nn.Module, model_input: \u0026#34;ModelInputForGPUWithSamplingMetadata\u0026#34;, kv_caches: List[torch.Tensor], hidden_or_intermediate_states: Union[torch.Tensor, IntermediateTensors], ) -\u0026gt; None: input_tokens_tensor = model_input.input_tokens seq_lens = model_input.attn_metadata.seq_lens slot_mapping_flat = model_input.attn_metadata.slot_mapping.flatten() num_prefill_tokens = model_input.attn_metadata.num_prefill_tokens start_layer = model_executable.model.start_layer end_layer = model_executable.model.end_layer num_heads, head_size = self.kv_helper.get_model_args(model_executable) # query_lens contains new KV caches that are added to vLLM. # so we will send them to decode instance # FIXME(Kuntai): This assume that all requests are prefill. for idx, slen in enumerate(seq_lens): start_pos = sum(seq_lens[:idx]) end_pos = start_pos + slen if start_pos \u0026gt;= num_prefill_tokens: # vllm/worker/model_runner.py::_prepare_model_input_tensors: # - input_tokens[:num_prefill_tokens] contains prefill tokens. # - input_tokens[num_prefill_tokens:] contains decode tokens. logger.warning(\u0026#34;You have some decode requests while using \u0026#34; \u0026#34;SimpleConnector. Their KVCache won\u0026#39;t be sent.\u0026#34;) break current_tokens = input_tokens_tensor[start_pos:end_pos] keys, values = [], [] for layer_id in range(start_layer, end_layer): kv_cache = kv_caches[layer_id - start_layer] key_cache, value_cache = self.kv_helper.get_kv_from_cache( kv_cache, num_heads, head_size) current_slot_mapping = slot_mapping_flat[start_pos:end_pos] keys.append(key_cache[current_slot_mapping].unsqueeze(0)) values.append(value_cache[current_slot_mapping].unsqueeze(0)) keys = torch.cat(keys, dim=0) values = torch.cat(values, dim=0) self.insert(current_tokens, torch.ones_like(current_tokens, dtype=bool), keys, values, hidden_or_intermediate_states[start_pos:end_pos]) logger.debug(\u0026#34;[rank%d]: KV send DONE.\u0026#34;, torch.distributed.get_rank()) def recv_kv_caches_and_hidden_states( self, model_executable: torch.nn.Module, model_input: \u0026#34;ModelInputForGPUWithSamplingMetadata\u0026#34;, kv_caches: List[torch.Tensor] ) -\u0026gt; Tuple[Union[torch.Tensor, IntermediateTensors], bool, \u0026#34;ModelInputForGPUWithSamplingMetadata\u0026#34;]: # When bypass_model_exec is set to False, it means that at least for one # request its corresponding KV cache or hidden state is missing. # In this case we need to do prefilling to recompute missing KV cache # and hidden states. bypass_model_exec = True input_tokens_tensor = model_input.input_tokens seq_lens = model_input.attn_metadata.seq_lens num_prefill_tokens = model_input.attn_metadata.num_prefill_tokens slot_mapping = model_input.attn_metadata.slot_mapping.flatten() start_layer = model_executable.model.start_layer end_layer = model_executable.model.end_layer hidden_or_intermediate_states_for_one_req = [] input_tokens_list = [] num_computed_tokens_list = [] start_pos_list = [] # enumerate different requests # FIXME(Kuntai): This impl assumes that all requests are prefill. for idx, slen in enumerate(seq_lens): start_pos = sum(seq_lens[:idx]) end_pos = start_pos + slen if start_pos \u0026gt;= num_prefill_tokens: # This can happen during inflight batching. See: # vllm/worker/model_runner.py::_prepare_model_input_tensors: # - input_tokens[:num_prefill_tokens] contains prefill tokens. # - input_tokens[num_prefill_tokens:] contains decode tokens. logger.warning(\u0026#34;You should set --enable_chunked_prefill=False \u0026#34; \u0026#34;and --max_num_batched_tokens \u0026#34; \u0026#34;should be equal to --max_seq_len_to_capture\u0026#34;) bypass_model_exec = False assert start_pos == num_prefill_tokens break current_tokens = input_tokens_tensor[start_pos:end_pos] num_tokens = slen # collecting data for rebuilding the input input_tokens_list.append(current_tokens) start_pos_list.append(start_pos) ret = self.select(current_tokens, torch.ones_like(current_tokens, dtype=bool)) if ret[0] is None: # didn\u0026#39;t find any match. bypass_model_exec = False num_computed_tokens_list.append(0) continue roi: torch.Tensor = ret[1] keys: torch.Tensor = ret[2] values: torch.Tensor = ret[3] hidden: torch.Tensor = ret[4] num_computed_tokens = roi.shape[0] num_computed_tokens_list.append(num_computed_tokens) # check if both KV cache and the hidden states are received # If not, need to redo the forwarding to compute missing states if not all([(num_computed_tokens == num_tokens), hidden is not None ]): bypass_model_exec = False # update the end position based on how many tokens are cached. end_pos = start_pos + num_computed_tokens 【KV caches 塞到paged memory 中】 # put received KV caches into paged memory for cur_layer in range(start_layer, end_layer): layer_id = cur_layer - start_layer kv_cache = kv_caches[layer_id] layer = model_executable.model.layers[cur_layer] # get remote kvcache remote_k, remote_v = keys[layer_id], values[layer_id] self.kv_helper.put_kv_to_cache(model_executable, remote_k, remote_v, layer, kv_cache, slot_mapping, start_pos, end_pos) hidden_or_intermediate_states_for_one_req.append(hidden) if not bypass_model_exec: # Some of the KV cache is not retrieved # Here we will fall back to normal model forwarding # But optionally you can adjust model_input so that you only do # prefilling on those tokens that are missing KV caches. logger.warning( \u0026#34;[rank%d]: Failed to receive all KVs and hidden \u0026#34; \u0026#34;states, redo model forwarding.\u0026#34;, torch.distributed.get_rank()) hidden_or_intermediate_states = None else: logger.debug( \u0026#34;[rank%d]: Successfully received all KVs and hidden \u0026#34; \u0026#34;states, skip model forwarding.\u0026#34;, torch.distributed.get_rank()) hidden_or_intermediate_states = torch.cat( hidden_or_intermediate_states_for_one_req, dim=0) return hidden_or_intermediate_states, bypass_model_exec, model_input 参考 # [EP03] 大模型推理，从vllm看PD分离\nvLLM PD分离方案浅析\n"},{"id":58,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/","title":"(实现)[vLLM]投机解码 +","section":"框架vLLM","content":"\n[vLLM]投机解码 # [vLLM]投机解码\n"},{"id":59,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/","title":"(实现)[vLLM]Prefix Caching +","section":"框架vLLM","content":"\nPrefix caching # llm.inference( input_tokens: list[int], # N tokens previous_kv_cache: list[Tensor], # N tokens\u0026#39; kv cache ∪ \u0026lt;N ) -\u0026gt; output_tokens, new_kv_cache output_tokens: # N\u0026#39; new tokens new_kv_cache: # kv cache of N + N\u0026#39; tokens Key: tokens Value: KV cache tensors\nclass KVCacheStore: def store(tokens, kv_cache_tensors): pass def retrieve(tokens) -\u0026gt; kv_cache_tensors: pass Prefix-based matching # Tokens 1: ABCDE -\u0026gt; [KV1, KV2, KV3, KV4, KV5] Tokens 2: ABCDF -\u0026gt; [KV1, KV2, KV3, KV4, KV6] kv_cache_store.store(\u0026#34;ABCDE\u0026#34;, [KV1, KV2, KV3, KV4, KV5]) kv_cache_store.retrieve(\u0026#34;ABCD\u0026#34;) -\u0026gt; [KV1, KV2, KV3, KV4] \u0026ldquo;Trie\u0026rdquo; \u0026ldquo;ABCDEF\u0026rdquo; -\u0026gt; \u0026ldquo;AB\u0026rdquo;, \u0026ldquo;CD\u0026rdquo;, \u0026ldquo;EF\u0026rdquo; -\u0026gt; list of chunked prefix hashes prefix_hash = \u0026#34;\u0026#34; for chunk in chunked_tokens: # [\u0026#34;AB\u0026#34;, \u0026#34;CD\u0026#34;, \u0026#34;EF\u0026#34;] chunk_hash = hash(prefix_hash + chunk) prefix_hash = chunk_hash # Given chunked prefix hashes, chunked kv cache # store for chunk_hash, chunk_kv in zip(...): redis.put(chunk_hash, chunk_kv) # retrieve for chunk_hash in ...: kv_chunk = redis.get(chunk_hash) if kv_chunk is None: break Eviction # LRU, LFU\u0026hellip; \u0026ldquo;ABCDEF\u0026rdquo; \u0026ndash;\u0026gt; [\u0026ldquo;AB\u0026rdquo;, KV1], [\u0026ldquo;CD\u0026rdquo;, KV2], [\u0026ldquo;EF\u0026rdquo;, KV3] 参考 # [EP05] vllm从开源到部署，Prefix Caching和开源答疑\n"},{"id":60,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMv1/","title":"(实现)[vLLM]V1 +","section":"框架vLLM","content":"\nvLLM V1 # (实现)[vLLM]V1\n"},{"id":61,"href":"/www6vMLSys/docs/LLMOps/MaaS/gptMaaSMonitor/","title":"MaaS 监控","section":"MaaS","content":"\nMaaS 监控 # MaaS 监控\n"},{"id":62,"href":"/www6vMLSys/docs/LLMOps/MaaS/gptLLMOpsPaaS/","title":"LLM PaaS","section":"MaaS","content":"\nLLM PaaS # LLM PaaS\n"},{"id":63,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/TP/TrainTensorParallelism/","title":"(原理)张量并行(TP) +","section":"TP","content":"\n张量并行(TP) # (原理)张量并行(TP)\n"},{"id":64,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/PP/TrainPipelineParallelism/","title":"(原理|实战)流水线并行(PP) +","section":"PP","content":"\n流水线并行(PP) # (原理|实战)流水线并行(PP)\n"},{"id":65,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/","title":"(原理)Streaming LLM +","section":"Sparse Attention","content":"\nStreaming LLM # Streaming LLM\n"},{"id":66,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/Overview/TrainParallelism/","title":"(原理)分布式训练 +","section":"Overview","content":"\n分布式训练 # (原理)分布式训练\n"}]