[{"id":0,"href":"/www6vMLSys/docs/LLMOps/GPU/gptGPUk8s/","title":"(实战)K8s部署GPU","section":"GPU","content":"\nK8s部署GPU # (实战)K8s部署GPU\n"},{"id":1,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Practice/gptQuantizationPractice/","title":"(实战)量化 +","section":"Practice","content":"\n量化实战 # (实战)量化-推理\n"},{"id":2,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/gptTrainMegatron/","title":"(原理)Megatron +","section":"混合并行","content":"\nMegatron # (原理)Megatron\n"},{"id":3,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantizationSurvey/","title":"(Survey)Quantization +","section":"Overview","content":"\nQuantization # A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms\n"},{"id":4,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationGPTQ/","title":"(原理|实战|实现)GPTQ +","section":"Weight Only","content":"\nGPTQ # (原理|实战|实现)GPTQ\n"},{"id":5,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationInt8/gptQuantizationInt8/","title":"(原理|实战)LLM.int8() +","section":"Weight\u0026Activation","content":"\nLLM.int8() # (原理|实战)LLM.int8()\n"},{"id":6,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/","title":"Speculative Decoding +","section":"SpeculativeDecoding","content":"\nSpeculative Decoding # Speculative Decoding\n"},{"id":7,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/","title":"DistServe +","section":"PD 分离","content":"\nDistServe # DistServe\n"},{"id":8,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/","title":"Continuous Batching +","section":"Batch","content":"\nContinuous Batching # Continuous Batching\n"},{"id":9,"href":"/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/gptLowPrecision/gptLowPrecision/","title":"低精度训练 +","section":"低精度","content":"\n低精度训练 # 低精度训练\n"},{"id":10,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptFlashAttention/","title":"(原理)Flash Attention +","section":"FlashAttention","content":"\nFlash Attention # (原理)Flash Attention\n"},{"id":11,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/","title":"(原理|实现) KV Cache +","section":"KVCache","content":"\nKV Cache # (原理|实现) KV Cache\n"},{"id":12,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/","title":"(原理) vLLM  +","section":"框架vLLM","content":"\nvLLM # (原理) vLLM\n"},{"id":13,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/gptQuantizationWeight/","title":"(原理)PTQ-Weight Only +","section":"PTQ","content":"\nWeight Only # Weight Only\n"},{"id":14,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/gptTrainZeroDeepspeed/","title":"(原理) Deepspeed Zero","section":"DP","content":"\nDeepspeed Zero # (原理) Deepspeed Zero\n"},{"id":15,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/","title":"(原理)推理-框架","section":"框架","content":"\n推理 框架[1] # inference execute engine(server)\nvLLM，TensorRT， deepspeed\ninference execute engine(pc/edge 移动端)\nllama.cpp\nmlc-llm\nollama\ninference Server\nTriton Server, Ray\nChat Server [2]\nFastChat, XInference, modelscope SWIFT\n参考 # 探秘LLM应用开发 8-19\nLLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference/FastChat等框架]\n1xx. 一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)\n1xx. 一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) 1xx. 大模型推理框架概述\n"},{"id":16,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInference/","title":"(总结)推理优化","section":"Overview","content":"\n推理 优化 # overview[2] # 有几种方法可以在内存中降低推理成本或/和加快推理速度。\n应用各种并行处理方式，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。 内存卸载，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。 智能批处理策略；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。 网络压缩技术，如修剪、量化、蒸馏。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。 针对目标模型架构的特定改进。许多架构变化，特别是针对注意力层的变化，有助于提高Transformer解码速度。 模型压缩 [1] # 剪枝（Pruning） 知识蒸馏（Knowledge Distillation，KD） 量化（Quantization） 低秩分解（Low-Rank Factorization） KV Cache # 参考 # 综述 # 一文探秘LLM应用开发(13)-模型部署与推理(优化理论) Large Transformer Model Inference Optimization lilianweng 1xx. NLP（十八）：LLM 的推理优化技术纵览 ***\n1xx. 大语言模型推理性能优化综述\n"},{"id":17,"href":"/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/gptPrecision/gptPrecision/","title":"(原理|实战)混合精度 +","section":"低精度","content":"\n混合精度 # (原理|实战)混合精度\n"},{"id":18,"href":"/www6vMLSys/docs/LLMOps/GPU/gptGPUMetrics/","title":"GPU 指标\u0026监控","section":"GPU","content":"\nGPU 指标\u0026amp;监控 # GPU 指标\u0026amp;监控\n"},{"id":19,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/gptTrainHybridParallel/","title":"(原理)混合并行 +","section":"混合并行","content":"\n混合并行 # (原理)混合并行\n"},{"id":20,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/","title":"(原理)FlashAttention2 +","section":"FlashAttention","content":"\nFlash Attention2 # (原理)Flash Attention2\n"},{"id":21,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/","title":"(Survey)Speculative Decoding +","section":"SpeculativeDecoding","content":"\nSpeculative Decoding # (Survey)Speculative Decoding "},{"id":22,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/","title":"(原理|实现)Mooncake +","section":"PD 分离","content":"\nMooncake # Mooncake\n"},{"id":23,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationSmoothQuant/gptQuantizationSmoothQuant/","title":"(原理)SmoothQuant +","section":"Weight\u0026Activation","content":"\nSmoothQuant # (原理)SmoothQuant\n"},{"id":24,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationAWQ/","title":"(原理|实战)AWQ +","section":"Weight Only","content":"\nAWQ # (原理|实战)AWQ\n"},{"id":25,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/","title":"Chunked Prefill +","section":"Batch","content":"\nChunked Prefill # Chunked Prefill\n"},{"id":26,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/","title":"(原理)KV Cache 优化 +","section":"KVCache","content":"\nKV Cache 优化 # (原理)KV cache优化\n"},{"id":27,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/","title":"(综述)推理优化 +","section":"Overview","content":"\n论文 # A Survey on Efficient Inference for Large Language Models 翻译\nA Survey on Efficient Inference for Large Language Models 总结\nInference Papers # Inference Papers\n"},{"id":28,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/","title":"(实战) vLLM +","section":"框架vLLM","content":"\nvLLM 实战 # (实战) vLLM\n"},{"id":29,"href":"/www6vMLSys/docs/LLMOps/gptLLamaFactory/","title":"LLama-Factory","section":"LLMOps","content":"\nLLama-Factory # LLama-Factory\n"},{"id":30,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/gptTrainDistributedPractice/","title":"(实战)DeepSpeed Training","section":"DP","content":"\nDeepSpeed Training # DeepSpeed Training\n"},{"id":31,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantization/","title":"(原理)量化 +","section":"Overview","content":"\n量化 # 量化\n"},{"id":32,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/","title":"(实战)推理-lmdeploy","section":"框架","content":"\nlmdeploy-推理部署 [10] # 模型转换 # TurboMind 推理+命令行本地对话 # TurboMind推理+API服务 # 启动服务 Client访问服务 参考 # lmdeploy 量化部署\n(5)LMDeploy 大模型量化部署实践 V 1xx. llm-action inference git\n"},{"id":33,"href":"/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/","title":"(综述)推理优化 +","section":"Overview","content":"\n论文 # Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems\n"},{"id":34,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/","title":"(原理)Llumnix +","section":"PD 分离","content":"\nLlumnix # (原理)Llumnix\n"},{"id":35,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/","title":"(实战)[vLLM]投机解码 +","section":"框架vLLM","content":"\n(实战)[vLLM]投机解码 # (实战)[vLLM]投机解码\n"},{"id":36,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/","title":"(原理|实现)Medusa +","section":"SpeculativeDecoding","content":"\nMedusa # (原理|实现)Medusa\n"},{"id":37,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationFP8/gptQuantizationFP8/","title":"(原理)FP8 +","section":"Weight\u0026Activation","content":"\nFP8 # (原理)FP8\n"},{"id":38,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/","title":"(原理)Flash Decoding +","section":"FlashAttention","content":"\nFlash Decoding # (原理)Flash Decoding\n"},{"id":39,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/gptTrainDDP/","title":"(原理|实战)DDP","section":"DP","content":"\nDDP # DDP\n"},{"id":40,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/","title":"(原理)KV Cache 量化 +","section":"Compress","content":"\nKV Cache 量化 # KV Cache 量化\n"},{"id":41,"href":"/www6vMLSys/docs/LLMOps/gptGPUComputing/","title":"显存估算","section":"LLMOps","content":"\n显存估算 # 显存估算\n"},{"id":42,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/","title":"(原理|实战) TensorRT-LLM +","section":"框架","content":"\nTensorRT-LLM # (原理|实战) TensorRT-LLM\n"},{"id":43,"href":"/www6vMLSys/docs/LLMOps/GPU/gptGPU/","title":"GPU 算力平台","section":"GPU","content":"\nGPU算力 # 免费[1] # modelscope 100小时 GPU 专业收费[2] # 显卡 # 显卡天梯榜 显卡天梯榜\n显卡 显卡 = GPU + 显存\n参考 # 5种在线GPU算力资源白嫖指南 V 5种专业在线GPU算力资源白嫖指南 V.\n1xx. 【PyTorch深度学习】01 GPU购买与白嫖指南 "},{"id":44,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/","title":"(原理|实战) [vLLM]Prefix Cache +","section":"框架vLLM","content":"\n(原理|实战) [vLLM]Prefix Cache # (原理|实战) [vLLM]Prefix Cache\n"},{"id":45,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/","title":"EAGLE +","section":"SpeculativeDecoding","content":"\nEAGLE # EAGLE\n"},{"id":46,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/gptTrainFSDP/","title":"(原理|实战)FSDP","section":"DP","content":"\nFSDP # FSDP\n"},{"id":47,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/","title":"(原理)推理 Ray","section":"框架","content":"\nArchitecture Overview # Application concepts [1] # Task - A remote function invocation. Object - An application value. Actor - a stateful worker process (an instance of a @ray.remote class). Driver - The program root, or the “main” program. Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment. Design [1] # Components One or more worker processes A raylet. scheduler object store head node Global Control Service (GCS) driver process(es) cluster-level services Spark vs. Ray[10] # 总的来说，Ray和Spark的主要差别在于他们的抽象层次。Spark对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。Ray的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，Ray揭示和暴露了并行，而Spark抽象和隐藏了并行。\n就架构而言，Spark采用BSP模型，是无副作用的，而Ray本质上是一个RPC 框架+Actor框架+对象存储。\n参考 # 1xx. 基于 Ray 的大规模离线推理 字节\n字节跳动基于 Ray 的大规模离线推理\n1xx. Ray Design Patterns 查看-\u0026gt;模式\n1xx. 大模型训练部署利器\u0026ndash;开源分布式计算框架Ray原理介绍\nSpark vs. Ray # 加州大学伯克利分校为何能连续孵化出 Mesos,Spark,Alluxio,Ray 等重量级开源项目? 孙挺Sunt 1xx. 分布式领域计算模型及Spark\u0026amp;Ray实现对比\nInternal # Ray v2 Architecture 1xx. Ray 分布式计算框架介绍\n1xx. Ray 1.0 架构解读\n"},{"id":48,"href":"/www6vMLSys/docs/LLMOps/gptLLMOps/","title":"LLMOps","section":"LLMOps","content":"\nLLMOps: Deployment and Learning in Production\nLLMOps: Deployment and Learning in Production\n[必读] LLM 应用开发全栈指南 LLMOps 了解一下新领域 LLMOps: 大模型运维\nUnderstanding LLMOps: Large Language Model Operations "},{"id":49,"href":"/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/","title":"SpecInfer +","section":"SpeculativeDecoding","content":"\nSpecInfer # SpecInfer\n"},{"id":50,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/","title":"(实战)推理 Ray","section":"框架","content":"\n实战 # 环境 # modelscope GPU\n实战1 # 脚本[1]\n遇到的异常[2]\n实战2 # 脚本 ### 变更模型名字 ### import \u0026#39;modelscope\u0026#39; package 异常[11] 实战3[20] # 脚本\nvllm 0.2.3 -\u0026gt; 报异常\nvllm 0.3.3 -\u0026gt; 报另一个异常 实战4 # 脚本 [30]\n异常 [31]\n# 运行这个命令报异常 python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000 monitor[40] # Ray Dashboard[41] # Ray logging # Loki grafana\nBuilt-in Ray Serve metrics # Prometheus\n参考 # 实战1 # Serve a Large Language Model with vLLM\nInvalid device id when using pytorch dataparallel！ 运行时碰到的异常\n实战2 # examples/offline_inference_distributed.py\n报错:RuntimeError: CUDA error: no kernel image is available for execution on the device\n实战3 # Ray vLLM Interence 1xx. GitHub - ray-project/langchain-ray: Examples on how to use LangChain and Ray git\n实战4 # 在甲骨文云上用 Ray +Vllm 部署 Mixtral 8*7B 模型_mixtral 8x7b 部署-CSDN博客\n报错:RuntimeError: CUDA error: no kernel image is available for execution on the device-CSDN博客\nmonitor # Monitor Your Application\nRay Dashboard "},{"id":51,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/","title":"(实现)[vLLM]整体架构 +","section":"框架vLLM","content":"\nvLLM # (实现)[vLLM]整体架构\n"},{"id":52,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/","title":"(实现)[vLLM]分布式 +","section":"框架vLLM","content":"\n[vLLM]分布式 # [vLLM]分布式 DP, TP, PP, EP\n"},{"id":53,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/","title":"(实现)[vLLM]PD分离 +","section":"框架vLLM","content":"\n[vLLM]PD分离 # [vLLM]PD分离\n"},{"id":54,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/","title":"(实现)[vLLM]投机解码 +","section":"框架vLLM","content":"\n[vLLM]投机解码 # [vLLM]投机解码\n"},{"id":55,"href":"/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/","title":"(实现)[vLLM]Prefix Caching","section":"框架vLLM","content":"\nPrefix Caching # Prefix Caching\n"},{"id":56,"href":"/www6vMLSys/docs/LLMOps/MaaS/gptMaaSMonitor/","title":"MaaS 监控","section":"MaaS","content":"\nMaaS 监控 # MaaS 监控\n"},{"id":57,"href":"/www6vMLSys/docs/LLMOps/MaaS/gptLLMOpsPaaS/","title":"LLM PaaS","section":"MaaS","content":"\nLLM PaaS # LLM PaaS\n"},{"id":58,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/TP/gptTrainTensorParallelism/","title":"(原理)张量并行(TP) +","section":"TP","content":"\n张量并行(TP) # (原理)张量并行(TP)\n"},{"id":59,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/PP/gptTrainPipelineParallelism/","title":"(原理|实战)流水线并行(PP) +","section":"PP","content":"\n流水线并行(PP) # (原理|实战)流水线并行(PP)\n"},{"id":60,"href":"/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/","title":"(原理)Streaming LLM +","section":"Sparse Attention","content":"\nStreaming LLM # Streaming LLM\n"},{"id":61,"href":"/www6vMLSys/docs/Inference-Opt/%E5%85%B6%E4%BB%96/gptTemperature/","title":"推理常见参数 +","section":"其他","content":"\n推理常见参数 # 推理常见参数\n"},{"id":62,"href":"/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/Overview/gptTrainParallelism/","title":"(原理)分布式训练","section":"Overview","content":"\n分布式训练 # (原理)分布式训练\n"}]