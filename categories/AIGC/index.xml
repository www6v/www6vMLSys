<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AIGC on MLSys</title>
    <link>https://www6v.github.io/www6vMLSys/categories/AIGC/</link>
    <description>Recent content in AIGC on MLSys</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 14 May 2025 19:53:42 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vMLSys/categories/AIGC/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>(实战)K8s部署GPU</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPUk8s/</link>
      <pubDate>Sat, 23 Mar 2024 12:43:50 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPUk8s/</guid>
      <description>K8s部署GPU # (实战)K8s部署GPU</description>
    </item>
    <item>
      <title>(实战)量化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Practice/gptQuantizationPractice/</link>
      <pubDate>Fri, 22 Mar 2024 10:18:01 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Practice/gptQuantizationPractice/</guid>
      <description>量化实战 # (实战)量化-推理</description>
    </item>
    <item>
      <title>(原理)Megatron &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainMegatron/</link>
      <pubDate>Sun, 26 Nov 2023 23:03:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainMegatron/</guid>
      <description>Megatron # (原理)Megatron</description>
    </item>
    <item>
      <title>(Survey)Quantization &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantizationSurvey/</link>
      <pubDate>Sat, 21 Oct 2023 21:26:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantizationSurvey/</guid>
      <description>Quantization # A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms</description>
    </item>
    <item>
      <title>(原理|实战|实现)GPTQ &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationGPTQ/</link>
      <pubDate>Thu, 12 Oct 2023 14:33:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationGPTQ/</guid>
      <description>GPTQ # (原理|实战|实现)GPTQ</description>
    </item>
    <item>
      <title>(原理|实战)LLM.int8() &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationInt8/gptQuantizationInt8/</link>
      <pubDate>Thu, 12 Oct 2023 14:33:20 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationInt8/gptQuantizationInt8/</guid>
      <description>LLM.int8() # (原理|实战)LLM.int8()</description>
    </item>
    <item>
      <title>Speculative Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/</link>
      <pubDate>Fri, 06 Oct 2023 17:40:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/</guid>
      <description>Speculative Decoding # Speculative Decoding</description>
    </item>
    <item>
      <title>DistServe &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/</link>
      <pubDate>Thu, 05 Oct 2023 22:43:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/</guid>
      <description>DistServe # DistServe</description>
    </item>
    <item>
      <title>Continuous Batching &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/</link>
      <pubDate>Mon, 18 Sep 2023 18:32:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/</guid>
      <description>Continuous Batching # Continuous Batching</description>
    </item>
    <item>
      <title>低精度训练 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/LowPrecision/gptLowPrecision/</link>
      <pubDate>Wed, 16 Aug 2023 11:50:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/LowPrecision/gptLowPrecision/</guid>
      <description>低精度训练 # 低精度训练</description>
    </item>
    <item>
      <title>(原理)Flash Attention &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptFlashAttention/</link>
      <pubDate>Tue, 13 Jun 2023 09:41:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptFlashAttention/</guid>
      <description>Flash Attention # (原理)Flash Attention</description>
    </item>
    <item>
      <title>(原理|实现) KV Cache &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/</link>
      <pubDate>Thu, 01 Jun 2023 11:09:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/</guid>
      <description>KV Cache # (原理|实现) KV Cache</description>
    </item>
    <item>
      <title>(原理) vLLM  &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/</link>
      <pubDate>Wed, 31 May 2023 22:24:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/</guid>
      <description>vLLM # (原理) vLLM</description>
    </item>
    <item>
      <title>(原理)PTQ-Weight Only &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/gptQuantizationWeight/</link>
      <pubDate>Sun, 26 Mar 2023 11:42:49 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/gptQuantizationWeight/</guid>
      <description>Weight Only # Weight Only</description>
    </item>
    <item>
      <title>(原理) Deepspeed Zero &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainZeroDeepspeed/</link>
      <pubDate>Thu, 23 Mar 2023 09:48:55 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainZeroDeepspeed/</guid>
      <description>Deepspeed Zero # (原理) Deepspeed Zero</description>
    </item>
    <item>
      <title>(原理)推理-框架</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/</link>
      <pubDate>Tue, 21 Mar 2023 22:18:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/</guid>
      <description>推理 框架[1] # inference execute engine(server)&#xA;vLLM，TensorRT， deepspeed&#xA;inference execute engine(pc/edge 移动端)&#xA;llama.cpp&#xA;mlc-llm&#xA;ollama&#xA;inference Server&#xA;Triton Server, Ray&#xA;Chat Server [2]&#xA;FastChat, XInference, modelscope SWIFT&#xA;参考 # 探秘LLM应用开发 8-19&#xA;LLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference/FastChat等框架]&#xA;1xx. 一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)&#xA;1xx. 一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) 1xx. 大模型推理框架概述</description>
    </item>
    <item>
      <title>(总结)推理优化</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInference/</link>
      <pubDate>Sun, 01 Jan 2023 22:58:43 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInference/</guid>
      <description>推理 优化 # overview[2] # 有几种方法可以在内存中降低推理成本或/和加快推理速度。&#xA;应用各种并行处理方式，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。 内存卸载，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。 智能批处理策略；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。 网络压缩技术，如修剪、量化、蒸馏。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。 针对目标模型架构的特定改进。许多架构变化，特别是针对注意力层的变化，有助于提高Transformer解码速度。 模型压缩 [1] # 剪枝（Pruning） 知识蒸馏（Knowledge Distillation，KD） 量化（Quantization） 低秩分解（Low-Rank Factorization） KV Cache # 参考 # 综述 # 一文探秘LLM应用开发(13)-模型部署与推理(优化理论) Large Transformer Model Inference Optimization lilianweng 1xx. NLP（十八）：LLM 的推理优化技术纵览 ***&#xA;1xx. 大语言模型推理性能优化综述</description>
    </item>
    <item>
      <title>(原理|实战)混合精度 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/Precision/gptPrecision/</link>
      <pubDate>Thu, 01 Feb 2024 22:29:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E4%BD%8E%E7%B2%BE%E5%BA%A6/Precision/gptPrecision/</guid>
      <description>混合精度 # (原理|实战)混合精度</description>
    </item>
    <item>
      <title>GPU 指标&amp;监控</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPUMetrics/</link>
      <pubDate>Sat, 23 Dec 2023 14:25:43 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPUMetrics/</guid>
      <description>GPU 指标&amp;amp;监控 # GPU 指标&amp;amp;监控</description>
    </item>
    <item>
      <title>(原理)混合并行 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainHybridParallel/</link>
      <pubDate>Sun, 26 Nov 2023 23:29:08 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/%E6%B7%B7%E5%90%88%E5%B9%B6%E8%A1%8C/TrainHybridParallel/</guid>
      <description>混合并行 # (原理)混合并行</description>
    </item>
    <item>
      <title>(原理)FlashAttention2 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/</link>
      <pubDate>Wed, 15 Nov 2023 18:55:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/</guid>
      <description>Flash Attention2 # (原理)Flash Attention2</description>
    </item>
    <item>
      <title>(Survey)Speculative Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/</link>
      <pubDate>Sat, 21 Oct 2023 21:12:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/</guid>
      <description>&#xA;Speculative Decoding # (Survey)Speculative Decoding </description>
    </item>
    <item>
      <title>(原理|实现)Mooncake &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/</link>
      <pubDate>Thu, 19 Oct 2023 12:02:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/</guid>
      <description>Mooncake # Mooncake</description>
    </item>
    <item>
      <title>(原理)SmoothQuant &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationSmoothQuant/gptQuantizationSmoothQuant/</link>
      <pubDate>Sat, 14 Oct 2023 13:33:21 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationSmoothQuant/gptQuantizationSmoothQuant/</guid>
      <description>SmoothQuant # (原理)SmoothQuant</description>
    </item>
    <item>
      <title>(原理|实战)AWQ &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationAWQ/</link>
      <pubDate>Thu, 12 Oct 2023 14:34:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/Weight-Only/gptQuantizationAWQ/</guid>
      <description>AWQ # (原理|实战)AWQ</description>
    </item>
    <item>
      <title>Chunked Prefill &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/</link>
      <pubDate>Mon, 18 Sep 2023 18:46:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/</guid>
      <description>Chunked Prefill # Chunked Prefill</description>
    </item>
    <item>
      <title>(原理)KV Cache 优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/</link>
      <pubDate>Sat, 02 Sep 2023 22:14:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/</guid>
      <description>KV Cache 优化 # (原理)KV cache优化</description>
    </item>
    <item>
      <title>(综述)推理优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/</link>
      <pubDate>Mon, 14 Aug 2023 13:23:31 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/</guid>
      <description>论文 # A Survey on Efficient Inference for Large Language Models 翻译&#xA;A Survey on Efficient Inference for Large Language Models 总结&#xA;Inference Papers # Inference Papers</description>
    </item>
    <item>
      <title>(实战) vLLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/</link>
      <pubDate>Mon, 12 Jun 2023 14:19:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/</guid>
      <description>vLLM 实战 # (实战) vLLM</description>
    </item>
    <item>
      <title>LLama-Factory</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/LLamaFactory/</link>
      <pubDate>Wed, 24 May 2023 18:43:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/LLamaFactory/</guid>
      <description>LLama-Factory # LLama-Factory</description>
    </item>
    <item>
      <title>(实战)DeepSpeed Training &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDistributedPractice/</link>
      <pubDate>Sat, 25 Mar 2023 15:55:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDistributedPractice/</guid>
      <description>DeepSpeed Training # DeepSpeed Training</description>
    </item>
    <item>
      <title>(原理)量化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantization/</link>
      <pubDate>Sun, 19 Feb 2023 17:00:25 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/Overview/gptQuantization/</guid>
      <description>量化 # 量化</description>
    </item>
    <item>
      <title>(实战)推理-lmdeploy</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/</link>
      <pubDate>Thu, 02 Feb 2023 11:14:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/</guid>
      <description>lmdeploy-推理部署 [10] # 模型转换 # TurboMind 推理+命令行本地对话 # TurboMind推理+API服务 # 启动服务 Client访问服务 参考 # lmdeploy 量化部署&#xA;(5)LMDeploy 大模型量化部署实践 V 1xx. llm-action inference git</description>
    </item>
    <item>
      <title>(综述)推理优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/</link>
      <pubDate>Wed, 11 Sep 2024 17:40:04 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/</guid>
      <description>论文 # Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</description>
    </item>
    <item>
      <title>(原理)Llumnix &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/</link>
      <pubDate>Thu, 30 Nov 2023 16:45:15 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/</guid>
      <description>Llumnix # (原理)Llumnix</description>
    </item>
    <item>
      <title>(实战)[vLLM]投机解码 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/</link>
      <pubDate>Sat, 18 Nov 2023 16:28:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/</guid>
      <description>(实战)[vLLM]投机解码 # (实战)[vLLM]投机解码</description>
    </item>
    <item>
      <title>(原理|实现)Medusa &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/</link>
      <pubDate>Sun, 12 Nov 2023 06:46:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/</guid>
      <description>Medusa # (原理|实现)Medusa</description>
    </item>
    <item>
      <title>(原理)FP8 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationFP8/gptQuantizationFP8/</link>
      <pubDate>Sat, 14 Oct 2023 13:33:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/%E9%87%8F%E5%8C%96/PTQ/WeightActivation/gptQuantizationFP8/gptQuantizationFP8/</guid>
      <description>FP8 # (原理)FP8</description>
    </item>
    <item>
      <title>(原理)Flash Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/</link>
      <pubDate>Fri, 06 Oct 2023 17:52:36 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/</guid>
      <description>Flash Decoding # (原理)Flash Decoding</description>
    </item>
    <item>
      <title>(原理|实战)DDP &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDDP/</link>
      <pubDate>Tue, 19 Sep 2023 18:08:48 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainDDP/</guid>
      <description>DDP # DDP</description>
    </item>
    <item>
      <title>(原理)KV Cache 量化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/</link>
      <pubDate>Sat, 02 Sep 2023 23:13:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/</guid>
      <description>KV Cache 量化 # KV Cache 量化</description>
    </item>
    <item>
      <title>显存估算</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPUComputing/</link>
      <pubDate>Sat, 01 Jul 2023 17:13:39 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPUComputing/</guid>
      <description>显存估算 # 显存估算</description>
    </item>
    <item>
      <title>(原理|实战) TensorRT-LLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/</link>
      <pubDate>Fri, 02 Jun 2023 21:59:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/</guid>
      <description>TensorRT-LLM # (原理|实战) TensorRT-LLM</description>
    </item>
    <item>
      <title>GPU 算力平台</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPU/</link>
      <pubDate>Tue, 23 May 2023 11:49:49 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/GPU/GPU/</guid>
      <description>&#xA;GPU算力 # 免费[1] # modelscope 100小时 GPU 专业收费[2] # 显卡 # 显卡天梯榜 显卡天梯榜&#xA;显卡 显卡 = GPU + 显存&#xA;参考 # 5种在线GPU算力资源白嫖指南 V 5种专业在线GPU算力资源白嫖指南 V.&#xA;1xx. 【PyTorch深度学习】01 GPU购买与白嫖指南 </description>
    </item>
    <item>
      <title>(原理|实战) [vLLM]Prefix Cache &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/</link>
      <pubDate>Fri, 15 Nov 2024 18:41:31 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/</guid>
      <description>(原理|实战) [vLLM]Prefix Cache # (原理|实战) [vLLM]Prefix Cache</description>
    </item>
    <item>
      <title>EAGLE &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/</link>
      <pubDate>Sun, 12 Nov 2023 06:46:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/</guid>
      <description>EAGLE # EAGLE</description>
    </item>
    <item>
      <title>(原理|实战)FSDP &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainFSDP/</link>
      <pubDate>Tue, 19 Sep 2023 18:17:13 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/DP/TrainFSDP/</guid>
      <description>FSDP # FSDP</description>
    </item>
    <item>
      <title>(原理)推理 Ray</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/</link>
      <pubDate>Sun, 11 Jun 2023 09:16:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/</guid>
      <description>Architecture Overview # Application concepts [1] # Task - A remote function invocation. Object - An application value. Actor - a stateful worker process (an instance of a @ray.remote class). Driver - The program root, or the “main” program. Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment. Design [1] # Components One or more worker processes A raylet. scheduler object store head node Global Control Service (GCS) driver process(es) cluster-level services Spark vs.</description>
    </item>
    <item>
      <title>LLMOps</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/LLMOps/</link>
      <pubDate>Wed, 28 Dec 2022 19:52:01 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/LLMOps/</guid>
      <description>&#xA;LLMOps: Deployment and Learning in Production&#xA;LLMOps: Deployment and Learning in Production&#xA;[必读] LLM 应用开发全栈指南 LLMOps 了解一下新领域 LLMOps: 大模型运维&#xA;Understanding LLMOps: Large Language Model Operations </description>
    </item>
    <item>
      <title>SpecInfer &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/</link>
      <pubDate>Sun, 12 Nov 2023 06:58:14 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/</guid>
      <description>SpecInfer # SpecInfer</description>
    </item>
    <item>
      <title>(实战)推理 Ray</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/</link>
      <pubDate>Fri, 16 Jun 2023 16:17:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/</guid>
      <description>实战 # 环境 # modelscope GPU&#xA;实战1 # 脚本[1]&#xA;遇到的异常[2]&#xA;实战2 # 脚本 ### 变更模型名字 ### import &amp;#39;modelscope&amp;#39; package 异常[11] 实战3[20] # 脚本&#xA;vllm 0.2.3 -&amp;gt; 报异常&#xA;vllm 0.3.3 -&amp;gt; 报另一个异常 实战4 # 脚本 [30]&#xA;异常 [31]&#xA;# 运行这个命令报异常 python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000 monitor[40] # Ray Dashboard[41] # Ray logging # Loki grafana&#xA;Built-in Ray Serve metrics # Prometheus</description>
    </item>
    <item>
      <title>(实现)[vLLM]整体架构 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/</link>
      <pubDate>Sat, 02 Dec 2023 15:09:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/</guid>
      <description>vLLM # (实现)[vLLM]整体架构</description>
    </item>
    <item>
      <title>(实现)[vLLM]分布式 *</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/</link>
      <pubDate>Tue, 14 May 2024 19:51:55 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/</guid>
      <description>Feature # Distributed Inference&#xA;Why distributed inference?&#xA;Infra-side&#xA;Communication device:&#xA;NVLink: direct communication between GPUs Infinity Band: High-speed connection between nodes RDMA: Remote direct memory access RDMA NIC Software solution Key advantage: bypass operating system / zero copy Communication library:&#xA;dlms/distributed/device_communicators PyDCL: communication for NVIDIA shared memory : OS custom allreduce - A kernel jsut for all reduce operation Before: 0 machine: [0] 1 machine: [1] 2 machine: [2] 3 machine: [3] After: 0 machine: [0,1,2,3] 1 machine: [0,1,2,3] 2 machine: [0,1,2,3] 3 machine: [0,1,2,3] torch.</description>
    </item>
    <item>
      <title>(实现)[vLLM]PD分离 *</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/</link>
      <pubDate>Tue, 14 May 2024 19:52:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/</guid>
      <description>PD disaggregation # What&amp;rsquo;s Prefill and Decode # prefill: process input prompt, generate KV cache decode: generate tokens based on the KV cache Why PD disaggregation # Prefill:&#xA;attention — N tokens QKV — generate KV cache takes a long time Decode:&#xA;attention N KV, 1 Q — generate a new token very fast initial logic&#xA;prioritize prefill&#xA;problem&#xA;prefill will stop other request&amp;rsquo;s decode&#xA;solution&#xA;PD disaggregation, chunked prefill</description>
    </item>
    <item>
      <title>(实现)[vLLM]投机解码 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/</link>
      <pubDate>Tue, 14 May 2024 19:53:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/</guid>
      <description>[vLLM]投机解码 # [vLLM]投机解码</description>
    </item>
    <item>
      <title>(实现)[vLLM]Prefix Caching &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/</link>
      <pubDate>Wed, 14 May 2025 19:53:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/</guid>
      <description>Prefix Caching # Prefix Caching</description>
    </item>
    <item>
      <title>(实现)[vLLM]V1 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMv1/</link>
      <pubDate>Tue, 14 May 2024 19:53:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMv1/</guid>
      <description>vLLM V1 # (实现)[vLLM]V1</description>
    </item>
    <item>
      <title>MaaS 监控</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/MaaS/gptMaaSMonitor/</link>
      <pubDate>Tue, 23 Apr 2024 21:54:43 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/MaaS/gptMaaSMonitor/</guid>
      <description>MaaS 监控 # MaaS 监控</description>
    </item>
    <item>
      <title>LLM PaaS</title>
      <link>https://www6v.github.io/www6vMLSys/docs/LLMOps/MaaS/gptLLMOpsPaaS/</link>
      <pubDate>Tue, 26 Sep 2023 23:18:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/LLMOps/MaaS/gptLLMOpsPaaS/</guid>
      <description>LLM PaaS # LLM PaaS</description>
    </item>
    <item>
      <title>(原理)张量并行(TP) &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/TP/TrainTensorParallelism/</link>
      <pubDate>Fri, 08 Sep 2023 19:11:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/TP/TrainTensorParallelism/</guid>
      <description>张量并行(TP) # (原理)张量并行(TP)</description>
    </item>
    <item>
      <title>(原理|实战)流水线并行(PP) &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/PP/TrainPipelineParallelism/</link>
      <pubDate>Fri, 08 Sep 2023 19:08:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/PP/TrainPipelineParallelism/</guid>
      <description>流水线并行(PP) # (原理|实战)流水线并行(PP)</description>
    </item>
    <item>
      <title>(原理)Streaming LLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/</link>
      <pubDate>Sat, 02 Sep 2023 23:07:02 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/</guid>
      <description>Streaming LLM # Streaming LLM</description>
    </item>
    <item>
      <title>推理常见参数 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E5%85%B6%E4%BB%96/gptTemperature/</link>
      <pubDate>Thu, 30 Mar 2023 23:25:05 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E5%85%B6%E4%BB%96/gptTemperature/</guid>
      <description>推理常见参数 # 推理常见参数</description>
    </item>
    <item>
      <title>(原理)分布式训练 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/Overview/TrainParallelism/</link>
      <pubDate>Fri, 06 Jan 2023 05:51:54 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Training/%E5%88%86%E5%B8%83%E5%BC%8F/Overview/TrainParallelism/</guid>
      <description>分布式训练 # (原理)分布式训练</description>
    </item>
  </channel>
</rss>
