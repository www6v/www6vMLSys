<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Infer on MLSys</title>
    <link>https://www6v.github.io/www6vMLSys/categories/Infer/</link>
    <description>Recent content in Infer on MLSys</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Wed, 14 May 2025 19:53:42 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vMLSys/categories/Infer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Speculative Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/</link>
      <pubDate>Fri, 06 Oct 2023 17:40:58 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecoding/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;speculative-decoding&#34;&gt;&#xA;  Speculative Decoding&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#speculative-decoding&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Speculative-Decoding-117bfe2110848060a00efd475a0abbac?pvs=4&#34;&gt;Speculative Decoding&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理) vLLM  &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/</link>
      <pubDate>Wed, 31 May 2023 22:24:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLM/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;vllm&#34;&gt;&#xA;  vLLM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vllm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/vLLM-ccb00d32fef14f92b0f7ab4c1c1db390?pvs=4&#34;&gt;(原理) vLLM&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)FlashAttention2 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/</link>
      <pubDate>Wed, 15 Nov 2023 18:55:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashAttention2/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;flash-attention2&#34;&gt;&#xA;  Flash Attention2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#flash-attention2&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Flash-Attention2-110bfe21108480f782fcc7258d860ccc?pvs=4&#34;&gt;(原理)Flash Attention2&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(Survey)Speculative Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/</link>
      <pubDate>Sat, 21 Oct 2023 21:12:51 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpeculativeDecodingSurvey/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xD;&#xA;&lt;!-- more --&gt;&#xD;&#xA;&lt;h1 id=&#34;speculative-decoding&#34;&gt;&#xA;  Speculative Decoding&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#speculative-decoding&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Unlocking-Efficiency-in-Large-Language-Model-Inference-A-Comprehensive-Survey-of-Speculative-Decodi-117bfe2110848059977ece3df3f9791d?pvs=4&#34;&gt;(Survey)Speculative Decoding &lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战) vLLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/</link>
      <pubDate>Mon, 12 Jun 2023 14:19:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;vllm-实战&#34;&gt;&#xA;  vLLM 实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vllm-%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/vllm-a35a50c4cd2c4875a8de173575275217?pvs=4&#34;&gt;(实战) vLLM&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)[vLLM]投机解码 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/</link>
      <pubDate>Sat, 18 Nov 2023 16:28:40 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferSpeculativeDecodingvLLM/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;speculating-with-a-draft-model&#34;&gt;&#xA;  &lt;strong&gt;Speculating with a draft model[1]&lt;/strong&gt;&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#speculating-with-a-draft-model&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; vllm &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; LLM, SamplingParams&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prompts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;The future of AI is&amp;#34;&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sampling_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SamplingParams(temperature&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt;, top_p&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.95&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; LLM(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;facebook/opt-6.7b&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;# verify 小模型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tensor_parallel_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    speculative_model&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;facebook/opt-125m&amp;#34;&lt;/span&gt;,    &lt;span style=&#34;color:#75715e&#34;&gt;# draft 小模型&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    num_speculative_tokens&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#75715e&#34;&gt;# 一次生成5个token &lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;outputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;generate(prompts, sampling_params)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; output &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; outputs:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prompt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prompt&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    generated_text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; output&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;outputs[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;text&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    print(&lt;span style=&#34;color:#e6db74&#34;&gt;f&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Prompt: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;prompt&lt;span style=&#34;color:#e6db74&#34;&gt;!r}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;, Generated text: &lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;{&lt;/span&gt;generated_text&lt;span style=&#34;color:#e6db74&#34;&gt;!r}&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;python -m vllm.entrypoints.openai.api_server --host &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;.0.0 --port &lt;span style=&#34;color:#ae81ff&#34;&gt;8000&lt;/span&gt; \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --model facebook/opt-&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;.7b \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --seed &lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt; -tp &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --speculative_model facebook/opt-&lt;span style=&#34;color:#ae81ff&#34;&gt;125m&lt;/span&gt; \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --use-v2-block-manager \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --num_speculative_tokens &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt; \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;       --gpu_memory_utilization &lt;span style=&#34;color:#ae81ff&#34;&gt;0.8&lt;/span&gt; \&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Small LM&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)Flash Decoding &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/</link>
      <pubDate>Fri, 06 Oct 2023 17:52:36 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/FlashAttention/gptInferFlashDecoding/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;flash-decoding&#34;&gt;&#xA;  Flash Decoding&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#flash-decoding&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/Flash-Decoding-110bfe2110848085aa6dde60217c486a?pvs=4&#34;&gt;(原理)Flash Decoding&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理|实战) TensorRT-LLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/</link>
      <pubDate>Fri, 02 Jun 2023 21:59:23 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferTensorRT/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;tensorrt-llm&#34;&gt;&#xA;  TensorRT-LLM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#tensorrt-llm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/TensorRT-LLM-11dbfe2110848030b7d5f27b9e9bda76?pvs=4&#34;&gt;(原理|实战) TensorRT-LLM&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(原理)推理 Ray</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/</link>
      <pubDate>Sun, 11 Jun 2023 09:16:45 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRay/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;architecture-overview&#34;&gt;&#xA;  Architecture Overview&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#architecture-overview&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;application-concepts-1&#34;&gt;&#xA;  Application concepts [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#application-concepts-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Task - A remote function invocation.&lt;/li&gt;&#xA;&lt;li&gt;Object - An application value.&lt;/li&gt;&#xA;&lt;li&gt;Actor - a stateful worker process (an instance of a &lt;code&gt;@ray.remote&lt;/code&gt; class).&lt;/li&gt;&#xA;&lt;li&gt;Driver - The program root, or the “main” program.&lt;/li&gt;&#xA;&lt;li&gt;Job - The collection of tasks, objects, and actors originating (recursively) from the same driver, and their runtime environment.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;design-1&#34;&gt;&#xA;  Design [1]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#design-1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Components&#xA;&lt;ul&gt;&#xA;&lt;li&gt;One or more worker processes&lt;/li&gt;&#xA;&lt;li&gt;A raylet.&#xA;&lt;ul&gt;&#xA;&lt;li&gt;scheduler&lt;/li&gt;&#xA;&lt;li&gt;object store&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;head node&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Global Control Service (GCS)&lt;/li&gt;&#xA;&lt;li&gt;driver process(es)&lt;/li&gt;&#xA;&lt;li&gt;cluster-level services&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;spark-vs-ray10&#34;&gt;&#xA;  Spark vs. Ray[10]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#spark-vs-ray10&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;总的来说，Ray和Spark的主要差别在于他们的&lt;strong&gt;抽象层次&lt;/strong&gt;。&lt;strong&gt;Spark&lt;/strong&gt;对并行进行抽象和限制，不允许用户编写真正并行的应用，从而使框架有更多的控制权。&lt;strong&gt;Ray&lt;/strong&gt;的层次要低得多，虽然给用户提供了更多灵活性，但更难编程。可以说，&lt;strong&gt;Ray揭示和暴露了并行，而Spark抽象和隐藏了并行&lt;/strong&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SpecInfer &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/</link>
      <pubDate>Sun, 12 Nov 2023 06:58:14 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferSpecInfer/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;specinfer&#34;&gt;&#xA;  SpecInfer&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#specinfer&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/SpecInfer-8b75821e44384cecba4541f7aa758adb?pvs=4&#34;&gt;SpecInfer&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实战)推理 Ray</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/</link>
      <pubDate>Fri, 16 Jun 2023 16:17:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferRayPractice/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;实战&#34;&gt;&#xA;  实战&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;环境&#34;&gt;&#xA;  环境&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e7%8e%af%e5%a2%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;modelscope  GPU&lt;/p&gt;&#xA;&lt;h3 id=&#34;实战1&#34;&gt;&#xA;  实战1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%981&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;脚本[1]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;遇到的异常[2]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实战2&#34;&gt;&#xA;  实战2&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%982&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;脚本&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-shell&#34; data-lang=&#34;shell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;### 变更模型名字&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;### import &amp;#39;modelscope&amp;#39; package&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;异常[11]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实战320&#34;&gt;&#xA;  实战3[20]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%98320&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;脚本&lt;br&gt;&#xA;vllm   0.2.3 -&amp;gt; 报异常&lt;br&gt;&#xA;vllm  0.3.3 -&amp;gt; 报另一个异常&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;实战4&#34;&gt;&#xA;  实战4&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%ae%9e%e6%88%984&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;脚本 [30]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;异常 [31]&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;# 运行这个命令报异常&#xA;python -m vllm.entrypoints.openai.api_server --trust-remote-code --served-model-name gpt-4 --model mistralai/Mixtral-8x7B-Instruct-v0.1 --gpu-memory-utilization 1 --tensor-parallel-size 8 --port 8000&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;monitor40&#34;&gt;&#xA;  monitor[40]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#monitor40&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;ray-dashboard41&#34;&gt;&#xA;  Ray Dashboard[41]&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ray-dashboard41&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;h3 id=&#34;ray-logging&#34;&gt;&#xA;  Ray logging&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#ray-logging&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;p&gt;Loki  grafana&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]整体架构 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/</link>
      <pubDate>Sat, 02 Dec 2023 15:09:52 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMCode/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;vllm&#34;&gt;&#xA;  vLLM&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vllm&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/vLLM-10bbfe2110848006b1f9d51397008e89?pvs=4&#34;&gt;(实现)[vLLM]整体架构&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]分布式 *</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/</link>
      <pubDate>Tue, 14 May 2024 19:51:55 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMDist/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h2 id=&#34;feature&#34;&gt;&#xA;  Feature&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#feature&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Distributed Inference&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Why distributed inference?&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Infra-side&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Communication device:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;NVLink: direct communication between GPUs&lt;/li&gt;&#xA;&lt;li&gt;Infinity Band: High-speed connection between nodes&lt;/li&gt;&#xA;&lt;li&gt;RDMA: Remote direct memory access&#xA;&lt;ul&gt;&#xA;&lt;li&gt;RDMA NIC&lt;/li&gt;&#xA;&lt;li&gt;Software solution&lt;/li&gt;&#xA;&lt;li&gt;Key advantage: bypass operating system / zero copy&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Communication library:&lt;/p&gt;&#xA;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dlms/distributed/device_communicators&#xA;&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;&#xA;&lt;li&gt;PyDCL: communication for NVIDIA&lt;/li&gt;&#xA;&lt;li&gt;shared memory : OS&lt;/li&gt;&#xA;&lt;li&gt;custom allreduce - A kernel jsut for all reduce operation&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Before:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;0 machine: [0]&lt;/li&gt;&#xA;&lt;li&gt;1 machine: [1]&lt;/li&gt;&#xA;&lt;li&gt;2 machine: [2]&lt;/li&gt;&#xA;&lt;li&gt;3 machine: [3]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;After:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;0 machine: [0,1,2,3]&lt;/li&gt;&#xA;&lt;li&gt;1 machine: [0,1,2,3]&lt;/li&gt;&#xA;&lt;li&gt;2 machine: [0,1,2,3]&lt;/li&gt;&#xA;&lt;li&gt;3 machine: [0,1,2,3]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;torch.distributed : provide wide support to a list of communication library&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;GroupCoordinator&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]PD分离 *</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/</link>
      <pubDate>Tue, 14 May 2024 19:52:38 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPD/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;pd-disaggregation&#34;&gt;&#xA;  PD disaggregation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#pd-disaggregation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;h3 id=&#34;whats-prefill-and-decode&#34;&gt;&#xA;  What&amp;rsquo;s Prefill and Decode&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#whats-prefill-and-decode&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;prefill:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;process input prompt, generate KV cache&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;decode:&#xA;&lt;ul&gt;&#xA;&lt;li&gt;generate tokens based on the KV cache&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h3 id=&#34;why-pd-disaggregation&#34;&gt;&#xA;  Why PD disaggregation&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#why-pd-disaggregation&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h3&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Prefill:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;attention — N tokens QKV — generate KV cache  takes a long time&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;Decode:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;attention N KV, 1 Q — generate a new token  very fast&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;initial logic&lt;/p&gt;&#xA;&lt;p&gt;prioritize prefill&lt;/p&gt;&#xA;&lt;/li&gt;&#xA;&lt;li&gt;&#xA;&lt;p&gt;problem&lt;/p&gt;&#xA;&lt;p&gt;&lt;strong&gt;prefill will stop other request&amp;rsquo;s decode&lt;/strong&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]投机解码 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/</link>
      <pubDate>Tue, 14 May 2024 19:53:12 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMSpeculativeDecode/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;vllm投机解码&#34;&gt;&#xA;  [vLLM]投机解码&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vllm%e6%8a%95%e6%9c%ba%e8%a7%a3%e7%a0%81&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/vLLM-1f4bfe211084804eb2a4e399bae5a91d?pvs=4&#34;&gt;[vLLM]投机解码&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]Prefix Caching &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/</link>
      <pubDate>Wed, 14 May 2025 19:53:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMPrefixCaching/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;prefix-caching&#34;&gt;&#xA;  Prefix caching&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prefix-caching&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;inference(&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    input_tokens: list[int],  &lt;span style=&#34;color:#75715e&#34;&gt;# N tokens&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    previous_kv_cache: list[Tensor],  &lt;span style=&#34;color:#75715e&#34;&gt;# N tokens&amp;#39; kv cache ∪ &amp;lt;N&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; output_tokens, new_kv_cache&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;output_tokens:  &lt;span style=&#34;color:#75715e&#34;&gt;# N&amp;#39; new tokens&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;new_kv_cache:  &lt;span style=&#34;color:#75715e&#34;&gt;# kv cache of N + N&amp;#39; tokens&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Key: tokens&#xA;Value: KV cache tensors&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;KVCacheStore&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;store&lt;/span&gt;(tokens, kv_cache_tensors):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;pass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;retrieve&lt;/span&gt;(tokens) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; kv_cache_tensors:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;pass&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;prefix-based-matching&#34;&gt;&#xA;  Prefix-based matching&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#prefix-based-matching&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Tokens 1: ABCDE -&amp;gt; [KV1, KV2, KV3, KV4, KV5]&lt;/li&gt;&#xA;&lt;li&gt;Tokens 2: ABCDF -&amp;gt; [KV1, KV2, KV3, KV4, KV6]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kv_cache_store&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;store(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ABCDE&amp;#34;&lt;/span&gt;, [KV1, KV2, KV3, KV4, KV5])&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kv_cache_store&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;retrieve(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ABCD&amp;#34;&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; [KV1, KV2, KV3, KV4]&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ul&gt;&#xA;&lt;li&gt;&amp;ldquo;Trie&amp;rdquo;&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;ABCDEF&amp;rdquo; -&amp;gt; &amp;ldquo;AB&amp;rdquo;, &amp;ldquo;CD&amp;rdquo;, &amp;ldquo;EF&amp;rdquo; -&amp;gt; list of chunked prefix hashes&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;prefix_hash &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; chunk &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; chunked_tokens:  &lt;span style=&#34;color:#75715e&#34;&gt;# [&amp;#34;AB&amp;#34;, &amp;#34;CD&amp;#34;, &amp;#34;EF&amp;#34;]&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    chunk_hash &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; hash(prefix_hash &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; chunk)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    prefix_hash &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; chunk_hash&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Given chunked prefix hashes, chunked kv cache&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# store&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; chunk_hash, chunk_kv &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;):&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    redis&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;put(chunk_hash, chunk_kv)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# retrieve&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; chunk_hash &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;...&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    kv_chunk &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; redis&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(chunk_hash)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; kv_chunk &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;None&lt;/span&gt;:&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;eviction&#34;&gt;&#xA;  Eviction&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#eviction&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;LRU, LFU&amp;hellip;&lt;/li&gt;&#xA;&lt;li&gt;&amp;ldquo;ABCDEF&amp;rdquo; &amp;ndash;&amp;gt; [&amp;ldquo;AB&amp;rdquo;, KV1], [&amp;ldquo;CD&amp;rdquo;, KV2], [&amp;ldquo;EF&amp;rdquo;, KV3]&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h1 id=&#34;参考&#34;&gt;&#xA;  参考&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#%e5%8f%82%e8%80%83&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://www.notion.so/EP05-vllm-Prefix-Caching-1fbbfe21108480db82c8d7cb6573eb5e?pvs=21&#34;&gt;&lt;strong&gt;[EP05] vllm从开源到部署，Prefix Caching和开源答疑&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>(实现)[vLLM]V1 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMv1/</link>
      <pubDate>Tue, 14 May 2024 19:53:42 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInfervLLMv1/</guid>
      <description>&lt;p&gt;&lt;/p&gt;&#xA;&lt;!-- more --&gt;&#xA;&lt;h1 id=&#34;vllm-v1&#34;&gt;&#xA;  vLLM V1&#xA;  &lt;a class=&#34;anchor&#34; href=&#34;#vllm-v1&#34;&gt;#&lt;/a&gt;&#xA;&lt;/h1&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://candied-skunk-1ca.notion.site/vLLM-V1-21bbfe21108480e697c8c4fdc1550ddc?source=copy_link&#34;&gt;(实现)[vLLM]V1&lt;/a&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
