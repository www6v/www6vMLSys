<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Inference on MLSys</title>
    <link>https://www6v.github.io/www6vMLSys/categories/Inference/</link>
    <description>Recent content in Inference on MLSys</description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Fri, 15 Nov 2024 18:41:31 +0000</lastBuildDate>
    <atom:link href="https://www6v.github.io/www6vMLSys/categories/Inference/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DistServe &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/</link>
      <pubDate>Thu, 05 Oct 2023 22:43:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferDistServe/</guid>
      <description>DistServe # DistServe</description>
    </item>
    <item>
      <title>Continuous Batching &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/</link>
      <pubDate>Mon, 18 Sep 2023 18:32:11 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferContinuousBatching/</guid>
      <description>Continuous Batching # Continuous Batching</description>
    </item>
    <item>
      <title>(原理|实现) KV Cache &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/</link>
      <pubDate>Thu, 01 Jun 2023 11:09:57 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCache/</guid>
      <description>KV Cache # (原理|实现) KV Cache</description>
    </item>
    <item>
      <title>(原理)推理-框架</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/</link>
      <pubDate>Tue, 21 Mar 2023 22:18:17 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFramework/</guid>
      <description>推理 框架[1] # inference execute engine(server)&#xA;vLLM，TensorRT， deepspeed&#xA;inference execute engine(pc/edge 移动端)&#xA;llama.cpp&#xA;mlc-llm&#xA;ollama&#xA;inference Server&#xA;Triton Server, Ray&#xA;Chat Server [2]&#xA;FastChat, XInference, modelscope SWIFT&#xA;参考 # 探秘LLM应用开发 8-19&#xA;LLM 大模型学习必知必会系列(十二)：VLLM性能飞跃部署实践：从推理加速到高效部署的全方位优化[更多内容：XInference/FastChat等框架]&#xA;1xx. 一文探秘LLM应用开发(18)-模型部署与推理(框架工具-Triton Server、RayLLM、OpenLLM)&#xA;1xx. 一文探秘LLM应用开发(16)-模型部署与推理(框架工具-TGI，vLLM，TensorRT-LLM，DS-MII) 1xx. 大模型推理框架概述</description>
    </item>
    <item>
      <title>(总结)推理优化</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInference/</link>
      <pubDate>Sun, 01 Jan 2023 22:58:43 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInference/</guid>
      <description>推理 优化 # overview[2] # 有几种方法可以在内存中降低推理成本或/和加快推理速度。&#xA;应用各种并行处理方式，以在大量GPU上扩展模型。智能并行处理模型组件和数据使得运行拥有数万亿参数的模型成为可能。 内存卸载，将临时未使用的数据卸载到CPU，并在以后需要时再读回。这有助于减少内存使用，但会导致更高的延迟。 智能批处理策略；例如，EffectiveTransformer将连续的序列打包在一起，以消除批处理内的填充。 网络压缩技术，如修剪、量化、蒸馏。较小的模型，无论是参数数量还是位宽，应该需要更少的内存并且运行更快。 针对目标模型架构的特定改进。许多架构变化，特别是针对注意力层的变化，有助于提高Transformer解码速度。 模型压缩 [1] # 剪枝（Pruning） 知识蒸馏（Knowledge Distillation，KD） 量化（Quantization） 低秩分解（Low-Rank Factorization） KV Cache # 参考 # 综述 # 一文探秘LLM应用开发(13)-模型部署与推理(优化理论) Large Transformer Model Inference Optimization lilianweng 1xx. NLP（十八）：LLM 的推理优化技术纵览 ***&#xA;1xx. 大语言模型推理性能优化综述</description>
    </item>
    <item>
      <title>(原理|实现)Mooncake &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/</link>
      <pubDate>Thu, 19 Oct 2023 12:02:53 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferMooncake/</guid>
      <description>Mooncake # Mooncake</description>
    </item>
    <item>
      <title>Chunked Prefill &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/</link>
      <pubDate>Mon, 18 Sep 2023 18:46:44 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/Batch/gptInferChunkedPrefill/</guid>
      <description>Chunked Prefill # Chunked Prefill</description>
    </item>
    <item>
      <title>(原理)KV Cache 优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/</link>
      <pubDate>Sat, 02 Sep 2023 22:14:46 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/gptInferKVCacheOptimize/</guid>
      <description>KV Cache 优化 # (原理)KV cache优化</description>
    </item>
    <item>
      <title>(综述)推理优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/</link>
      <pubDate>Mon, 14 Aug 2023 13:23:31 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey/</guid>
      <description>论文 # A Survey on Efficient Inference for Large Language Models 翻译&#xA;A Survey on Efficient Inference for Large Language Models 总结&#xA;Inference Papers # Inference Papers</description>
    </item>
    <item>
      <title>(实战)推理-lmdeploy</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/</link>
      <pubDate>Thu, 02 Feb 2023 11:14:35 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6/gptInferFrameworkPractice/</guid>
      <description>lmdeploy-推理部署 [10] # 模型转换 # TurboMind 推理+命令行本地对话 # TurboMind推理+API服务 # 启动服务 Client访问服务 参考 # lmdeploy 量化部署&#xA;(5)LMDeploy 大模型量化部署实践 V 1xx. llm-action inference git</description>
    </item>
    <item>
      <title>(综述)推理优化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/</link>
      <pubDate>Wed, 11 Sep 2024 17:40:04 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/Overview/gptInferenceSurvey1/</guid>
      <description>论文 # Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems</description>
    </item>
    <item>
      <title>(原理)Llumnix &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/</link>
      <pubDate>Thu, 30 Nov 2023 16:45:15 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/PD-%E5%88%86%E7%A6%BB/gptInferLlumnix/</guid>
      <description>Llumnix # (原理)Llumnix</description>
    </item>
    <item>
      <title>(原理|实现)Medusa &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/</link>
      <pubDate>Sun, 12 Nov 2023 06:46:10 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferMedusa/</guid>
      <description>Medusa # (原理|实现)Medusa</description>
    </item>
    <item>
      <title>(原理)KV Cache 量化 &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/</link>
      <pubDate>Sat, 02 Sep 2023 23:13:07 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/KVCache/Compress/gptInferKVCacheQuantization/</guid>
      <description>KV Cache 量化 # KV Cache 量化</description>
    </item>
    <item>
      <title>(原理|实战) [vLLM]Prefix Cache &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/</link>
      <pubDate>Fri, 15 Nov 2024 18:41:31 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference/%E6%A1%86%E6%9E%B6vLLM/gptInferKVCacheRadixAttention/</guid>
      <description>(原理|实战) [vLLM]Prefix Cache # (原理|实战) [vLLM]Prefix Cache</description>
    </item>
    <item>
      <title>EAGLE &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/</link>
      <pubDate>Sun, 12 Nov 2023 06:46:22 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E7%B3%BB%E7%BB%9F%E5%B1%82%E4%BC%98%E5%8C%96/SpeculativeDecoding/gptInferEagle/</guid>
      <description>EAGLE # EAGLE</description>
    </item>
    <item>
      <title>(原理)Streaming LLM &#43;</title>
      <link>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/</link>
      <pubDate>Sat, 02 Sep 2023 23:07:02 +0000</pubDate>
      <guid>https://www6v.github.io/www6vMLSys/docs/Inference-Opt/%E6%A8%A1%E5%9E%8B%E5%B1%82%E4%BC%98%E5%8C%96/Sparse-Attention/gptInferKVCacheStreamingLLM/</guid>
      <description>Streaming LLM # Streaming LLM</description>
    </item>
  </channel>
</rss>
